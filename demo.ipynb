{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/videorecap/lib/python3.8/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/videorecap/lib/python3.8/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms._transforms_video as transforms_video\n",
    "from transformers import AutoTokenizer\n",
    "from moviepy.editor import *\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "from src.data.video_transforms import Permute\n",
    "from src.models.video_recap import VideoRecap\n",
    "from src.data.datasets import VideoCaptionDataset, CaptionDataCollator\n",
    "from src.models.timesformer import SpaceTimeTransformer\n",
    "from src.models.openai_model import QuickGELU\n",
    "from src.configs.defaults import defaultConfigs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"assets/clientVid1.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "vid = 'clientVid1'\n",
    "video_file = f'assets/{vid}.mp4'\n",
    "Video(video_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clip Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Creating model\n",
      "######USING ATTENTION STYLE:  frozen-in-time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate_query.dense.bias', 'encoder.layer.0.intermediate_query.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output_query.LayerNorm.bias', 'encoder.layer.0.output_query.LayerNorm.weight', 'encoder.layer.0.output_query.dense.bias', 'encoder.layer.0.output_query.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate_query.dense.bias', 'encoder.layer.1.intermediate_query.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output_query.LayerNorm.bias', 'encoder.layer.1.output_query.LayerNorm.weight', 'encoder.layer.1.output_query.dense.bias', 'encoder.layer.1.output_query.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate_query.dense.bias', 'encoder.layer.10.intermediate_query.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output_query.LayerNorm.bias', 'encoder.layer.10.output_query.LayerNorm.weight', 'encoder.layer.10.output_query.dense.bias', 'encoder.layer.10.output_query.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate_query.dense.bias', 'encoder.layer.11.intermediate_query.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output_query.LayerNorm.bias', 'encoder.layer.11.output_query.LayerNorm.weight', 'encoder.layer.11.output_query.dense.bias', 'encoder.layer.11.output_query.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate_query.dense.bias', 'encoder.layer.2.intermediate_query.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output_query.LayerNorm.bias', 'encoder.layer.2.output_query.LayerNorm.weight', 'encoder.layer.2.output_query.dense.bias', 'encoder.layer.2.output_query.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate_query.dense.bias', 'encoder.layer.3.intermediate_query.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output_query.LayerNorm.bias', 'encoder.layer.3.output_query.LayerNorm.weight', 'encoder.layer.3.output_query.dense.bias', 'encoder.layer.3.output_query.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate_query.dense.bias', 'encoder.layer.4.intermediate_query.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output_query.LayerNorm.bias', 'encoder.layer.4.output_query.LayerNorm.weight', 'encoder.layer.4.output_query.dense.bias', 'encoder.layer.4.output_query.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate_query.dense.bias', 'encoder.layer.5.intermediate_query.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output_query.LayerNorm.bias', 'encoder.layer.5.output_query.LayerNorm.weight', 'encoder.layer.5.output_query.dense.bias', 'encoder.layer.5.output_query.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate_query.dense.bias', 'encoder.layer.6.intermediate_query.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output_query.LayerNorm.bias', 'encoder.layer.6.output_query.LayerNorm.weight', 'encoder.layer.6.output_query.dense.bias', 'encoder.layer.6.output_query.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate_query.dense.bias', 'encoder.layer.7.intermediate_query.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output_query.LayerNorm.bias', 'encoder.layer.7.output_query.LayerNorm.weight', 'encoder.layer.7.output_query.dense.bias', 'encoder.layer.7.output_query.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate_query.dense.bias', 'encoder.layer.8.intermediate_query.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output_query.LayerNorm.bias', 'encoder.layer.8.output_query.LayerNorm.weight', 'encoder.layer.8.output_query.dense.bias', 'encoder.layer.8.output_query.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate_query.dense.bias', 'encoder.layer.9.intermediate_query.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output_query.LayerNorm.bias', 'encoder.layer.9.output_query.LayerNorm.weight', 'encoder.layer.9.output_query.dense.bias', 'encoder.layer.9.output_query.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze the pretrained parts in Bert: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Learn the rest parts in Bert: ['bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias']\n",
      "Freeze the pretrained parts in Bert: 273\n",
      "Learn the rest parts in Bert: 60\n",
      "Freeze the pretrained parts in LM: 100\n",
      "Learn the rest parts in LM: 240\n",
      "=> loaded resume checkpoint 'pretrained_models/videorecap/videorecap_clip.pt' (epoch 5)\n"
     ]
    }
   ],
   "source": [
    "# Create model and tokenizer\n",
    "ckpt_path = 'pretrained_models/videorecap/videorecap_clip.pt'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "old_args = ckpt['args']\n",
    "old_args.video_feature_type = 'pixel'  \n",
    "old_args.num_video_feat=4                     # number of frames per clip caption\n",
    "crop_size = 224\n",
    "transform = transforms.Compose([\n",
    "        Permute([3, 0, 1, 2]),  # T H W C -> C T H W\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms_video.NormalizeVideo(mean=[108.3272985, 116.7460125, 104.09373615000001], std=[68.5005327, 66.6321579, 70.32316305]),\n",
    "    ])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(old_args.decoder_name)\n",
    "state_dict = OrderedDict()\n",
    "for k, v in ckpt['state_dict'].items():\n",
    "    state_dict[k.replace('module.', '')] = v\n",
    "\n",
    "print(\"=> Creating model\")\n",
    "model = VideoRecap(old_args, eval_only=True)\n",
    "model = model.cuda()\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "print(\"=> loaded resume checkpoint '{}' (epoch {})\".format(ckpt_path, ckpt['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video length 147.8 seconds\n",
      "number of captions 37\n",
      "37 5\n"
     ]
    }
   ],
   "source": [
    "# Create dataset from the video\n",
    "video = VideoFileClip(video_file)\n",
    "print('Video length', video.duration, 'seconds')\n",
    "\n",
    "video_length = video.duration\n",
    "caption_duration = 4                              # Extract clip caption at each 4 seconds\n",
    "old_args.video_loader_type='moviepy'\n",
    "old_args.chunk_len = -1                           # load from raw video\n",
    "old_args.video_feature_path = 'assets'            # path to the video folder \n",
    "metadata = []  \n",
    "for i in np.arange(0, video_length, caption_duration):\n",
    "    metadata.append([vid, i, min(i + caption_duration, video_length)])    # video name is example.mp4 so assuming video id=example\n",
    "print('number of captions', len(metadata))\n",
    "\n",
    "old_args.metadata = metadata\n",
    "dataset = VideoCaptionDataset(old_args, transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=False, \n",
    "                                        num_workers=8, pin_memory=True, drop_last=False)\n",
    "print(len(dataset), len(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption decoding function\n",
    "def decode_one(generated_ids, tokenizer):\n",
    "    if tokenizer.eos_token_id == tokenizer.bos_token_id:\n",
    "        if tokenizer.eos_token_id in generated_ids[1:].tolist():\n",
    "            eos_id = generated_ids[1:].tolist().index(tokenizer.eos_token_id) + 1\n",
    "        else:\n",
    "            eos_id = len(generated_ids.tolist()) - 1\n",
    "    elif tokenizer.eos_token_id in generated_ids.tolist():\n",
    "        eos_id = generated_ids.tolist().index(tokenizer.eos_token_id)\n",
    "    else:\n",
    "        eos_id = len(generated_ids.tolist()) - 1\n",
    "    generated_text_str = tokenizer.decode(generated_ids[1:eos_id].tolist())\n",
    "    return generated_text_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 4.0 #C C walks down the stairs\n",
      "4.0 8.0 #C C adjusts the camera\n",
      "8.0 12.0 #C C opens the door\n",
      "12.0 16.0 #C C passes the cup to her left hand\n",
      "16.0 20.0 #C C drinks from the mug\n",
      "20.0 24.0 #C C walks on the field\n",
      "24.0 28.0 #C C picks a shoe\n",
      "28.0 32.0 #C C rides a bicycle\n",
      "32.0 36.0 #C C looks around\n",
      "36.0 40.0 #C C looks around the lake\n",
      "40.0 44.0 #C C looks around\n",
      "44.0 48.0 #C C rides the bicycle\n",
      "48.0 52.0 #C C points at the man X with his right hand\n",
      "52.0 56.0 #C C points at the board\n",
      "56.0 60.0 #C C passes the paper to his right hand.\n",
      "60.0 64.0 #C C looks around\n",
      "64.0 68.0 #C C looks around\n",
      "68.0 72.0 #C C looks around\n",
      "72.0 76.0 #C C climbs down the mountain\n",
      "76.0 80.0 #C C takes a picture with the phone in her hands\n",
      "80.0 84.0 #C C looks around\n",
      "84.0 88.0 #C C runs on the field\n",
      "88.0 92.0 #C C walks on the lake\n",
      "92.0 96.0 #C C looks around\n",
      "96.0 100.0 #C C looks around\n",
      "100.0 104.0 #C C looks around\n",
      "104.0 108.0 #C C opens a door with her right hand.\n",
      "108.0 112.0 #C C looks around.\n",
      "112.0 116.0 #C C looks around\n",
      "116.0 120.0 #C C looks around\n",
      "120.0 124.0 #C C looks around the area\n",
      "124.0 128.0 #C C takes the seeds from the stove with the stick in her right hand\n",
      "128.0 132.0 #C C lights the fire\n",
      "132.0 136.0 #C C looks around the area\n",
      "136.0 140.0 #C C looks around\n",
      "140.0 144.0 #C C looks around the compound\n",
      "144.0 147.8 #C C looks at the mirror\n"
     ]
    }
   ],
   "source": [
    "captions = {}\n",
    "with torch.no_grad():\n",
    "    for data_iter, samples in enumerate(data_loader):\n",
    "        indices = samples['index']\n",
    "        if hasattr(model, \"vision_model\"):\n",
    "            image = samples[\"video_features\"].permute(0, 2, 1, 3, 4).contiguous().cuda()  # BCTHW -> BTCHW\n",
    "            samples[\"video_features\"] = model.vision_model.forward_features(image, use_checkpoint=old_args.use_checkpoint, cls_at_last=False)  # NLD\n",
    "        \n",
    "        queries = model.map_features(samples)\n",
    "    \n",
    "        if old_args.caption_sample == 'multinomial_sample':\n",
    "            generated_text_ids, ppls = model.generate(\n",
    "                queries,\n",
    "                tokenizer,\n",
    "                do_sample = False,\n",
    "                max_text_length=old_args.max_gen_tokens,\n",
    "                num_return_sequences=old_args.caption_num_return_sequences,\n",
    "            )\n",
    "            \n",
    "        for j in range(generated_text_ids.shape[0]):\n",
    "            sample = dataset.samples[indices[j].item()]\n",
    "            start_sec = sample[1]\n",
    "            for k in range(old_args.caption_num_return_sequences):\n",
    "                jj = j * old_args.caption_num_return_sequences + k\n",
    "                generated_text_str = decode_one(generated_text_ids[jj], tokenizer).strip()\n",
    "                captions[start_sec] = sample + [generated_text_str]\n",
    "                print(sample[1], sample[2], generated_text_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = {}\n",
    "with torch.no_grad():\n",
    "    for data_iter, samples in enumerate(tqdm(data_loader)):\n",
    "        image = samples[\"video_features\"].permute(0, 2, 1, 3, 4).contiguous().cuda()  # BCTHW -> BTCHW\n",
    "        features = model.vision_model.forward_features(image, cls_at_last=True)  # NLD\n",
    "        for j in range(features.shape[0]):\n",
    "            start_sec = dataset.samples[samples['index'][j].item()][1]\n",
    "            all_features[start_sec] = features[j].detach().cpu().numpy()\n",
    "            # print(start_sec, all_features[start_sec].shape)\n",
    "           \n",
    "seconds = list(all_features.keys())\n",
    "seconds.sort()\n",
    "features = []\n",
    "for s in seconds:\n",
    "    features.append(all_features[s])\n",
    "features = np.stack(features)\n",
    "print(features.shape)\n",
    "np.save(f'assets/{vid}.npy', features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'pretrained_models/videorecap/videorecap_segment.pt'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "old_args = ckpt['args']\n",
    "old_args.video_feature_type = 'cls'\n",
    "old_args.video_feature_path = 'assets'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(old_args.decoder_name)\n",
    "collator = CaptionDataCollator(tokenizer, max_gen_tokens = old_args.max_gen_tokens,\n",
    "                                add_bos = True, add_eos = True, pad_token_id = 0)\n",
    "state_dict = OrderedDict()\n",
    "for k, v in ckpt['state_dict'].items():\n",
    "    state_dict[k.replace('module.', '')] = v\n",
    "    \n",
    "print(\"=> Creating model\")\n",
    "model = VideoRecap(old_args, eval_only=True)\n",
    "model = model.cuda()\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "print(\"=> loaded resume checkpoint '{}' (epoch {})\".format(ckpt_path, ckpt['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = VideoFileClip(video_file)\n",
    "print('Video length', video.duration, 'seconds')\n",
    "\n",
    "old_args.video_feature_path = 'assets' \n",
    "video_length = video.duration\n",
    "segment_step = 180        # Extract one segment description at each 180 sconds\n",
    "metadata = []\n",
    "for i in np.arange(0, video_length, segment_step):\n",
    "    dd = {}\n",
    "    dd['vid'] = vid\n",
    "    dd['start_sec'] = i\n",
    "    dd['end_sec'] = min(i+segment_step, video_length)\n",
    "    dd['captions_pred'] = []\n",
    "    for s, c in captions.items():\n",
    "        if c[1]>=dd['start_sec'] and c[2]<=dd['end_sec']:\n",
    "            dd['captions_pred'].append(c)\n",
    "    metadata.append(dd)\n",
    "print('Number of segments', len(metadata))\n",
    "    \n",
    "old_args.metadata = metadata\n",
    "dataset = VideoCaptionDataset(old_args, transform=None)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False, \n",
    "                    collate_fn=collator, num_workers=4, pin_memory=True, drop_last=False)\n",
    "print(len(dataset), len(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_descriptions = {}\n",
    "with torch.no_grad():\n",
    "    for data_iter, samples in enumerate(data_loader):\n",
    "        indices = samples['indices']\n",
    "        if hasattr(model, \"vision_model\"):\n",
    "            image = samples[\"video_features\"].permute(0, 2, 1, 3, 4).contiguous().cuda()  # BCTHW -> BTCHW\n",
    "            samples[\"video_features\"] = model.vision_model.forward_features(image, use_checkpoint=old_args.use_checkpoint, cls_at_last=False)  # NLD\n",
    "        \n",
    "        queries = model.map_features(samples)\n",
    "    \n",
    "        if old_args.caption_sample == 'multinomial_sample':\n",
    "            generated_text_ids, ppls = model.generate(\n",
    "                queries,\n",
    "                tokenizer,\n",
    "                do_sample = False,\n",
    "                max_text_length=old_args.max_gen_tokens,\n",
    "                num_return_sequences=old_args.caption_num_return_sequences,\n",
    "            )\n",
    "            \n",
    "        for j in range(generated_text_ids.shape[0]):\n",
    "            sample = dataset.samples[indices[j].item()]\n",
    "            start_sec = sample['start_sec']\n",
    "            for k in range(old_args.caption_num_return_sequences):\n",
    "                jj = j * old_args.caption_num_return_sequences + k\n",
    "                generated_text_str = decode_one(generated_text_ids[jj], tokenizer).strip()\n",
    "                segment_descriptions[start_sec] = generated_text_str\n",
    "                print(sample['start_sec'], sample['end_sec'], generated_text_str)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'pretrained_models/videorecap/videorecap_video.pt'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "old_args = ckpt['args']\n",
    "old_args.video_feature_type = 'cls'\n",
    "old_args.video_feature_path = 'assets'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(old_args.decoder_name)\n",
    "collator = CaptionDataCollator(tokenizer, max_gen_tokens = old_args.max_gen_tokens,\n",
    "                                add_bos = True, add_eos = True, pad_token_id = 0)\n",
    "state_dict = OrderedDict()\n",
    "for k, v in ckpt['state_dict'].items():\n",
    "    state_dict[k.replace('module.', '')] = v\n",
    "    \n",
    "print(\"=> Creating model\")\n",
    "model = VideoRecap(old_args, eval_only=True)\n",
    "model = model.cuda()\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "print(\"=> loaded resume checkpoint '{}' (epoch {})\".format(ckpt_path, ckpt['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = VideoFileClip(video_file)\n",
    "print('Video length', video.duration, 'seconds')\n",
    "\n",
    "old_args.video_feature_path = 'assets' \n",
    "video_length = video.duration\n",
    "metadata = []\n",
    "dd = {}\n",
    "dd['vid'] = vid\n",
    "dd['start_sec'] = 0\n",
    "dd['end_sec'] = video.duration\n",
    "dd['segment_descriptions_pred'] = []\n",
    "for s, c in segment_descriptions.items():\n",
    "    dd['segment_descriptions_pred'].append(c)\n",
    "metadata.append(dd)\n",
    "print('Number of segments', len(metadata))\n",
    "    \n",
    "old_args.metadata = metadata\n",
    "dataset = VideoCaptionDataset(old_args, transform=None)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, \n",
    "                    collate_fn=collator, num_workers=1, pin_memory=True, drop_last=False)\n",
    "print(len(dataset), len(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_summary = {}\n",
    "with torch.no_grad():\n",
    "    for data_iter, samples in enumerate(data_loader):\n",
    "        indices = samples['indices']\n",
    "        if hasattr(model, \"vision_model\"):\n",
    "            image = samples[\"video_features\"].permute(0, 2, 1, 3, 4).contiguous().cuda()  # BCTHW -> BTCHW\n",
    "            samples[\"video_features\"] = model.vision_model.forward_features(image, use_checkpoint=old_args.use_checkpoint, cls_at_last=False)  # NLD\n",
    "        \n",
    "        queries = model.map_features(samples)\n",
    "    \n",
    "        if old_args.caption_sample == 'multinomial_sample':\n",
    "            generated_text_ids, ppls = model.generate(\n",
    "                queries,\n",
    "                tokenizer,\n",
    "                do_sample = False,\n",
    "                max_text_length=old_args.max_gen_tokens,\n",
    "                num_return_sequences=old_args.caption_num_return_sequences,\n",
    "            )\n",
    "            \n",
    "        for j in range(generated_text_ids.shape[0]):\n",
    "            sample = dataset.samples[indices[j].item()]\n",
    "            start_sec = sample['start_sec']\n",
    "            for k in range(old_args.caption_num_return_sequences):\n",
    "                jj = j * old_args.caption_num_return_sequences + k\n",
    "                generated_text_str = decode_one(generated_text_ids[jj], tokenizer).strip()\n",
    "                video_summary[start_sec] = generated_text_str\n",
    "                print(generated_text_str)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

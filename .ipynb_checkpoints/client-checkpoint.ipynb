{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c78deac-d13f-4332-9ff4-b29d1c519bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: ['label']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "#Replace with the server's actual IP and port\n",
    "url = \"http://127.0.0.1:8000/process\"\n",
    "\n",
    "#Path to the image you want to upload\n",
    "image_file = \"assets/framework.png\"  # Replace with the path to your image file\n",
    "\n",
    "#Open the image file and send it as part of a POST request\n",
    "with open(image_file, \"rb\") as f:\n",
    "    files = {\"file\": f}\n",
    "    response = requests.post(url, files=files)\n",
    "\n",
    "#Print the response from the server\n",
    "if response.status_code == 200:\n",
    "    print(\"Label:\", response.json())\n",
    "else:\n",
    "    print(\"Failed to process image:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b250e5a-21d2-4b00-b056-f0645e61082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT EDIT !!!\n",
    "\n",
    "# import requests\n",
    "\n",
    "\n",
    "# url = \"http://127.0.0.1:8000/upload-video/\"  # Replace <server-ip> with the server's IP\n",
    "# #files = {'file': open('test.jpeg', 'rb')}  # Replace 'image.jpg' with your file path\n",
    "\n",
    "# # response = requests.post(url, files=files)\n",
    "# # if response.status_code == 200:\n",
    "# #     print(\"Result:\", response.json())\n",
    "# # else:\n",
    "# #     print(\"Error:\", response.text)\n",
    "\n",
    "# # Specify the video file to upload\n",
    "# video_file = \"clientVid2.mp4\"  # Replace with your local video file path\n",
    "\n",
    "# # Upload the video file\n",
    "# with open(video_file, \"rb\") as f:\n",
    "#     files = {\"file\": f}\n",
    "#     response = requests.post(url, files=files)\n",
    "\n",
    "# # Print the server's response\n",
    "# if response.status_code == 200:\n",
    "#     print(\"Upload successful:\", response.json())\n",
    "# else:\n",
    "#     print(\"Upload failed:\", response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7290e38-56bf-4e73-8d65-c7b7ee34d5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in c:\\users\\jaxuan\\.conda\\envs\\videorecap\\lib\\site-packages (0.115.7)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in c:\\users\\jaxuan\\.conda\\envs\\videorecap\\lib\\site-packages (from fastapi) (0.44.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\jaxuan\\.conda\\envs\\videorecap\\lib\\site-packages (from fastapi) (2.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\jaxuan\\.conda\\envs\\videorecap\\lib\\site-packages (from fastapi) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jaxuan\\.conda\\envs\\videorecap\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\jaxuan\\.conda\\envs\\videorecap\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\jaxuan\\.conda\\envs\\videorecap\\lib\\site-packages (from starlette<0.46.0,>=0.40.0->fastapi) (4.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jaxuan\\.conda\\envs\\videorecap\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.46.0,>=0.40.0->fastapi) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\jaxuan\\.conda\\envs\\videorecap\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.46.0,>=0.40.0->fastapi) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\jaxuan\\.conda\\envs\\videorecap\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.46.0,>=0.40.0->fastapi) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07eb9492-f31c-428e-bdb1-24eef1dd3a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaxuan\\.conda\\envs\\videorecap\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jaxuan\\.conda\\envs\\videorecap\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, File, UploadFile\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from fastapi.responses import JSONResponse\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms._transforms_video as transforms_video\n",
    "from transformers import AutoTokenizer\n",
    "from moviepy.editor import *\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "from src.data.video_transforms import Permute\n",
    "from src.models.video_recap import VideoRecap\n",
    "from src.data.datasets import VideoCaptionDataset, CaptionDataCollator\n",
    "from src.models.timesformer import SpaceTimeTransformer\n",
    "from src.models.openai_model import QuickGELU\n",
    "from src.configs.defaults import defaultConfigs\n",
    "\n",
    "import requests\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "203ecb0d-79fa-4d3a-adda-13c384eda705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save uploaded videos\n",
    "UPLOAD_DIRECTORY = \"assets\"\n",
    "os.makedirs(UPLOAD_DIRECTORY, exist_ok=True)\n",
    "\n",
    "# Specify the video file to upload\n",
    "video_file = \"clientVid1\"  # Replace with your local video file path\n",
    "\n",
    "# Save the uploaded video file\n",
    "# file_path = os.path.join(UPLOAD_DIRECTORY, file.filename)\n",
    "# with open(file_path, \"wb\") as f:\n",
    "#     f.write(await file.read())\n",
    "#vid = os.path.splitext(file.filename)[0]\n",
    "vid = video_file #temporary solution, needs to be fixed\n",
    "video_file = f'assets/{vid}.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de15d6ef-0978-461e-8bb9-5a11f16aca74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Creating model\n",
      "######USING ATTENTION STYLE:  frozen-in-time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate_query.dense.bias', 'encoder.layer.0.intermediate_query.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output_query.LayerNorm.bias', 'encoder.layer.0.output_query.LayerNorm.weight', 'encoder.layer.0.output_query.dense.bias', 'encoder.layer.0.output_query.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate_query.dense.bias', 'encoder.layer.1.intermediate_query.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output_query.LayerNorm.bias', 'encoder.layer.1.output_query.LayerNorm.weight', 'encoder.layer.1.output_query.dense.bias', 'encoder.layer.1.output_query.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate_query.dense.bias', 'encoder.layer.10.intermediate_query.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output_query.LayerNorm.bias', 'encoder.layer.10.output_query.LayerNorm.weight', 'encoder.layer.10.output_query.dense.bias', 'encoder.layer.10.output_query.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate_query.dense.bias', 'encoder.layer.11.intermediate_query.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output_query.LayerNorm.bias', 'encoder.layer.11.output_query.LayerNorm.weight', 'encoder.layer.11.output_query.dense.bias', 'encoder.layer.11.output_query.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate_query.dense.bias', 'encoder.layer.2.intermediate_query.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output_query.LayerNorm.bias', 'encoder.layer.2.output_query.LayerNorm.weight', 'encoder.layer.2.output_query.dense.bias', 'encoder.layer.2.output_query.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate_query.dense.bias', 'encoder.layer.3.intermediate_query.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output_query.LayerNorm.bias', 'encoder.layer.3.output_query.LayerNorm.weight', 'encoder.layer.3.output_query.dense.bias', 'encoder.layer.3.output_query.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate_query.dense.bias', 'encoder.layer.4.intermediate_query.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output_query.LayerNorm.bias', 'encoder.layer.4.output_query.LayerNorm.weight', 'encoder.layer.4.output_query.dense.bias', 'encoder.layer.4.output_query.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate_query.dense.bias', 'encoder.layer.5.intermediate_query.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output_query.LayerNorm.bias', 'encoder.layer.5.output_query.LayerNorm.weight', 'encoder.layer.5.output_query.dense.bias', 'encoder.layer.5.output_query.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate_query.dense.bias', 'encoder.layer.6.intermediate_query.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output_query.LayerNorm.bias', 'encoder.layer.6.output_query.LayerNorm.weight', 'encoder.layer.6.output_query.dense.bias', 'encoder.layer.6.output_query.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate_query.dense.bias', 'encoder.layer.7.intermediate_query.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output_query.LayerNorm.bias', 'encoder.layer.7.output_query.LayerNorm.weight', 'encoder.layer.7.output_query.dense.bias', 'encoder.layer.7.output_query.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate_query.dense.bias', 'encoder.layer.8.intermediate_query.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output_query.LayerNorm.bias', 'encoder.layer.8.output_query.LayerNorm.weight', 'encoder.layer.8.output_query.dense.bias', 'encoder.layer.8.output_query.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate_query.dense.bias', 'encoder.layer.9.intermediate_query.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output_query.LayerNorm.bias', 'encoder.layer.9.output_query.LayerNorm.weight', 'encoder.layer.9.output_query.dense.bias', 'encoder.layer.9.output_query.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze the pretrained parts in Bert: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Learn the rest parts in Bert: ['bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias']\n",
      "Freeze the pretrained parts in Bert: 273\n",
      "Learn the rest parts in Bert: 60\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VideoRecap:\n\tUnexpected key(s) in state_dict: \"text_decoder.transformer.wte.weight\", \"text_decoder.transformer.wpe.weight\", \"text_decoder.transformer.h.0.alpha_cattn\", \"text_decoder.transformer.h.0.alpha_dense\", \"text_decoder.transformer.h.0.ln_1.weight\", \"text_decoder.transformer.h.0.ln_1.bias\", \"text_decoder.transformer.h.0.attn.bias\", \"text_decoder.transformer.h.0.attn.masked_bias\", \"text_decoder.transformer.h.0.attn.c_attn.weight\", \"text_decoder.transformer.h.0.attn.c_attn.bias\", \"text_decoder.transformer.h.0.attn.c_proj.weight\", \"text_decoder.transformer.h.0.attn.c_proj.bias\", \"text_decoder.transformer.h.0.ln_2.weight\", \"text_decoder.transformer.h.0.ln_2.bias\", \"text_decoder.transformer.h.0.crossattention.bias\", \"text_decoder.transformer.h.0.crossattention.masked_bias\", \"text_decoder.transformer.h.0.crossattention.c_attn.weight\", \"text_decoder.transformer.h.0.crossattention.c_attn.bias\", \"text_decoder.transformer.h.0.crossattention.q_attn.weight\", \"text_decoder.transformer.h.0.crossattention.q_attn.bias\", \"text_decoder.transformer.h.0.crossattention.c_proj.weight\", \"text_decoder.transformer.h.0.crossattention.c_proj.bias\", \"text_decoder.transformer.h.0.ln_cross_attn.weight\", \"text_decoder.transformer.h.0.ln_cross_attn.bias\", \"text_decoder.transformer.h.0.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.0.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.0.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.0.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.0.ln_2_crossattention.weight\", \"text_decoder.transformer.h.0.ln_2_crossattention.bias\", \"text_decoder.transformer.h.0.mlp.c_fc.weight\", \"text_decoder.transformer.h.0.mlp.c_fc.bias\", \"text_decoder.transformer.h.0.mlp.c_proj.weight\", \"text_decoder.transformer.h.0.mlp.c_proj.bias\", \"text_decoder.transformer.h.1.alpha_cattn\", \"text_decoder.transformer.h.1.alpha_dense\", \"text_decoder.transformer.h.1.ln_1.weight\", \"text_decoder.transformer.h.1.ln_1.bias\", \"text_decoder.transformer.h.1.attn.bias\", \"text_decoder.transformer.h.1.attn.masked_bias\", \"text_decoder.transformer.h.1.attn.c_attn.weight\", \"text_decoder.transformer.h.1.attn.c_attn.bias\", \"text_decoder.transformer.h.1.attn.c_proj.weight\", \"text_decoder.transformer.h.1.attn.c_proj.bias\", \"text_decoder.transformer.h.1.ln_2.weight\", \"text_decoder.transformer.h.1.ln_2.bias\", \"text_decoder.transformer.h.1.crossattention.bias\", \"text_decoder.transformer.h.1.crossattention.masked_bias\", \"text_decoder.transformer.h.1.crossattention.c_attn.weight\", \"text_decoder.transformer.h.1.crossattention.c_attn.bias\", \"text_decoder.transformer.h.1.crossattention.q_attn.weight\", \"text_decoder.transformer.h.1.crossattention.q_attn.bias\", \"text_decoder.transformer.h.1.crossattention.c_proj.weight\", \"text_decoder.transformer.h.1.crossattention.c_proj.bias\", \"text_decoder.transformer.h.1.ln_cross_attn.weight\", \"text_decoder.transformer.h.1.ln_cross_attn.bias\", \"text_decoder.transformer.h.1.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.1.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.1.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.1.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.1.ln_2_crossattention.weight\", \"text_decoder.transformer.h.1.ln_2_crossattention.bias\", \"text_decoder.transformer.h.1.mlp.c_fc.weight\", \"text_decoder.transformer.h.1.mlp.c_fc.bias\", \"text_decoder.transformer.h.1.mlp.c_proj.weight\", \"text_decoder.transformer.h.1.mlp.c_proj.bias\", \"text_decoder.transformer.h.2.alpha_cattn\", \"text_decoder.transformer.h.2.alpha_dense\", \"text_decoder.transformer.h.2.ln_1.weight\", \"text_decoder.transformer.h.2.ln_1.bias\", \"text_decoder.transformer.h.2.attn.bias\", \"text_decoder.transformer.h.2.attn.masked_bias\", \"text_decoder.transformer.h.2.attn.c_attn.weight\", \"text_decoder.transformer.h.2.attn.c_attn.bias\", \"text_decoder.transformer.h.2.attn.c_proj.weight\", \"text_decoder.transformer.h.2.attn.c_proj.bias\", \"text_decoder.transformer.h.2.ln_2.weight\", \"text_decoder.transformer.h.2.ln_2.bias\", \"text_decoder.transformer.h.2.crossattention.bias\", \"text_decoder.transformer.h.2.crossattention.masked_bias\", \"text_decoder.transformer.h.2.crossattention.c_attn.weight\", \"text_decoder.transformer.h.2.crossattention.c_attn.bias\", \"text_decoder.transformer.h.2.crossattention.q_attn.weight\", \"text_decoder.transformer.h.2.crossattention.q_attn.bias\", \"text_decoder.transformer.h.2.crossattention.c_proj.weight\", \"text_decoder.transformer.h.2.crossattention.c_proj.bias\", \"text_decoder.transformer.h.2.ln_cross_attn.weight\", \"text_decoder.transformer.h.2.ln_cross_attn.bias\", \"text_decoder.transformer.h.2.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.2.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.2.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.2.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.2.ln_2_crossattention.weight\", \"text_decoder.transformer.h.2.ln_2_crossattention.bias\", \"text_decoder.transformer.h.2.mlp.c_fc.weight\", \"text_decoder.transformer.h.2.mlp.c_fc.bias\", \"text_decoder.transformer.h.2.mlp.c_proj.weight\", \"text_decoder.transformer.h.2.mlp.c_proj.bias\", \"text_decoder.transformer.h.3.alpha_cattn\", \"text_decoder.transformer.h.3.alpha_dense\", \"text_decoder.transformer.h.3.ln_1.weight\", \"text_decoder.transformer.h.3.ln_1.bias\", \"text_decoder.transformer.h.3.attn.bias\", \"text_decoder.transformer.h.3.attn.masked_bias\", \"text_decoder.transformer.h.3.attn.c_attn.weight\", \"text_decoder.transformer.h.3.attn.c_attn.bias\", \"text_decoder.transformer.h.3.attn.c_proj.weight\", \"text_decoder.transformer.h.3.attn.c_proj.bias\", \"text_decoder.transformer.h.3.ln_2.weight\", \"text_decoder.transformer.h.3.ln_2.bias\", \"text_decoder.transformer.h.3.crossattention.bias\", \"text_decoder.transformer.h.3.crossattention.masked_bias\", \"text_decoder.transformer.h.3.crossattention.c_attn.weight\", \"text_decoder.transformer.h.3.crossattention.c_attn.bias\", \"text_decoder.transformer.h.3.crossattention.q_attn.weight\", \"text_decoder.transformer.h.3.crossattention.q_attn.bias\", \"text_decoder.transformer.h.3.crossattention.c_proj.weight\", \"text_decoder.transformer.h.3.crossattention.c_proj.bias\", \"text_decoder.transformer.h.3.ln_cross_attn.weight\", \"text_decoder.transformer.h.3.ln_cross_attn.bias\", \"text_decoder.transformer.h.3.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.3.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.3.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.3.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.3.ln_2_crossattention.weight\", \"text_decoder.transformer.h.3.ln_2_crossattention.bias\", \"text_decoder.transformer.h.3.mlp.c_fc.weight\", \"text_decoder.transformer.h.3.mlp.c_fc.bias\", \"text_decoder.transformer.h.3.mlp.c_proj.weight\", \"text_decoder.transformer.h.3.mlp.c_proj.bias\", \"text_decoder.transformer.h.4.alpha_cattn\", \"text_decoder.transformer.h.4.alpha_dense\", \"text_decoder.transformer.h.4.ln_1.weight\", \"text_decoder.transformer.h.4.ln_1.bias\", \"text_decoder.transformer.h.4.attn.bias\", \"text_decoder.transformer.h.4.attn.masked_bias\", \"text_decoder.transformer.h.4.attn.c_attn.weight\", \"text_decoder.transformer.h.4.attn.c_attn.bias\", \"text_decoder.transformer.h.4.attn.c_proj.weight\", \"text_decoder.transformer.h.4.attn.c_proj.bias\", \"text_decoder.transformer.h.4.ln_2.weight\", \"text_decoder.transformer.h.4.ln_2.bias\", \"text_decoder.transformer.h.4.crossattention.bias\", \"text_decoder.transformer.h.4.crossattention.masked_bias\", \"text_decoder.transformer.h.4.crossattention.c_attn.weight\", \"text_decoder.transformer.h.4.crossattention.c_attn.bias\", \"text_decoder.transformer.h.4.crossattention.q_attn.weight\", \"text_decoder.transformer.h.4.crossattention.q_attn.bias\", \"text_decoder.transformer.h.4.crossattention.c_proj.weight\", \"text_decoder.transformer.h.4.crossattention.c_proj.bias\", \"text_decoder.transformer.h.4.ln_cross_attn.weight\", \"text_decoder.transformer.h.4.ln_cross_attn.bias\", \"text_decoder.transformer.h.4.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.4.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.4.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.4.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.4.ln_2_crossattention.weight\", \"text_decoder.transformer.h.4.ln_2_crossattention.bias\", \"text_decoder.transformer.h.4.mlp.c_fc.weight\", \"text_decoder.transformer.h.4.mlp.c_fc.bias\", \"text_decoder.transformer.h.4.mlp.c_proj.weight\", \"text_decoder.transformer.h.4.mlp.c_proj.bias\", \"text_decoder.transformer.h.5.alpha_cattn\", \"text_decoder.transformer.h.5.alpha_dense\", \"text_decoder.transformer.h.5.ln_1.weight\", \"text_decoder.transformer.h.5.ln_1.bias\", \"text_decoder.transformer.h.5.attn.bias\", \"text_decoder.transformer.h.5.attn.masked_bias\", \"text_decoder.transformer.h.5.attn.c_attn.weight\", \"text_decoder.transformer.h.5.attn.c_attn.bias\", \"text_decoder.transformer.h.5.attn.c_proj.weight\", \"text_decoder.transformer.h.5.attn.c_proj.bias\", \"text_decoder.transformer.h.5.ln_2.weight\", \"text_decoder.transformer.h.5.ln_2.bias\", \"text_decoder.transformer.h.5.crossattention.bias\", \"text_decoder.transformer.h.5.crossattention.masked_bias\", \"text_decoder.transformer.h.5.crossattention.c_attn.weight\", \"text_decoder.transformer.h.5.crossattention.c_attn.bias\", \"text_decoder.transformer.h.5.crossattention.q_attn.weight\", \"text_decoder.transformer.h.5.crossattention.q_attn.bias\", \"text_decoder.transformer.h.5.crossattention.c_proj.weight\", \"text_decoder.transformer.h.5.crossattention.c_proj.bias\", \"text_decoder.transformer.h.5.ln_cross_attn.weight\", \"text_decoder.transformer.h.5.ln_cross_attn.bias\", \"text_decoder.transformer.h.5.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.5.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.5.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.5.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.5.ln_2_crossattention.weight\", \"text_decoder.transformer.h.5.ln_2_crossattention.bias\", \"text_decoder.transformer.h.5.mlp.c_fc.weight\", \"text_decoder.transformer.h.5.mlp.c_fc.bias\", \"text_decoder.transformer.h.5.mlp.c_proj.weight\", \"text_decoder.transformer.h.5.mlp.c_proj.bias\", \"text_decoder.transformer.h.6.alpha_cattn\", \"text_decoder.transformer.h.6.alpha_dense\", \"text_decoder.transformer.h.6.ln_1.weight\", \"text_decoder.transformer.h.6.ln_1.bias\", \"text_decoder.transformer.h.6.attn.bias\", \"text_decoder.transformer.h.6.attn.masked_bias\", \"text_decoder.transformer.h.6.attn.c_attn.weight\", \"text_decoder.transformer.h.6.attn.c_attn.bias\", \"text_decoder.transformer.h.6.attn.c_proj.weight\", \"text_decoder.transformer.h.6.attn.c_proj.bias\", \"text_decoder.transformer.h.6.ln_2.weight\", \"text_decoder.transformer.h.6.ln_2.bias\", \"text_decoder.transformer.h.6.crossattention.bias\", \"text_decoder.transformer.h.6.crossattention.masked_bias\", \"text_decoder.transformer.h.6.crossattention.c_attn.weight\", \"text_decoder.transformer.h.6.crossattention.c_attn.bias\", \"text_decoder.transformer.h.6.crossattention.q_attn.weight\", \"text_decoder.transformer.h.6.crossattention.q_attn.bias\", \"text_decoder.transformer.h.6.crossattention.c_proj.weight\", \"text_decoder.transformer.h.6.crossattention.c_proj.bias\", \"text_decoder.transformer.h.6.ln_cross_attn.weight\", \"text_decoder.transformer.h.6.ln_cross_attn.bias\", \"text_decoder.transformer.h.6.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.6.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.6.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.6.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.6.ln_2_crossattention.weight\", \"text_decoder.transformer.h.6.ln_2_crossattention.bias\", \"text_decoder.transformer.h.6.mlp.c_fc.weight\", \"text_decoder.transformer.h.6.mlp.c_fc.bias\", \"text_decoder.transformer.h.6.mlp.c_proj.weight\", \"text_decoder.transformer.h.6.mlp.c_proj.bias\", \"text_decoder.transformer.h.7.alpha_cattn\", \"text_decoder.transformer.h.7.alpha_dense\", \"text_decoder.transformer.h.7.ln_1.weight\", \"text_decoder.transformer.h.7.ln_1.bias\", \"text_decoder.transformer.h.7.attn.bias\", \"text_decoder.transformer.h.7.attn.masked_bias\", \"text_decoder.transformer.h.7.attn.c_attn.weight\", \"text_decoder.transformer.h.7.attn.c_attn.bias\", \"text_decoder.transformer.h.7.attn.c_proj.weight\", \"text_decoder.transformer.h.7.attn.c_proj.bias\", \"text_decoder.transformer.h.7.ln_2.weight\", \"text_decoder.transformer.h.7.ln_2.bias\", \"text_decoder.transformer.h.7.crossattention.bias\", \"text_decoder.transformer.h.7.crossattention.masked_bias\", \"text_decoder.transformer.h.7.crossattention.c_attn.weight\", \"text_decoder.transformer.h.7.crossattention.c_attn.bias\", \"text_decoder.transformer.h.7.crossattention.q_attn.weight\", \"text_decoder.transformer.h.7.crossattention.q_attn.bias\", \"text_decoder.transformer.h.7.crossattention.c_proj.weight\", \"text_decoder.transformer.h.7.crossattention.c_proj.bias\", \"text_decoder.transformer.h.7.ln_cross_attn.weight\", \"text_decoder.transformer.h.7.ln_cross_attn.bias\", \"text_decoder.transformer.h.7.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.7.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.7.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.7.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.7.ln_2_crossattention.weight\", \"text_decoder.transformer.h.7.ln_2_crossattention.bias\", \"text_decoder.transformer.h.7.mlp.c_fc.weight\", \"text_decoder.transformer.h.7.mlp.c_fc.bias\", \"text_decoder.transformer.h.7.mlp.c_proj.weight\", \"text_decoder.transformer.h.7.mlp.c_proj.bias\", \"text_decoder.transformer.h.8.alpha_cattn\", \"text_decoder.transformer.h.8.alpha_dense\", \"text_decoder.transformer.h.8.ln_1.weight\", \"text_decoder.transformer.h.8.ln_1.bias\", \"text_decoder.transformer.h.8.attn.bias\", \"text_decoder.transformer.h.8.attn.masked_bias\", \"text_decoder.transformer.h.8.attn.c_attn.weight\", \"text_decoder.transformer.h.8.attn.c_attn.bias\", \"text_decoder.transformer.h.8.attn.c_proj.weight\", \"text_decoder.transformer.h.8.attn.c_proj.bias\", \"text_decoder.transformer.h.8.ln_2.weight\", \"text_decoder.transformer.h.8.ln_2.bias\", \"text_decoder.transformer.h.8.crossattention.bias\", \"text_decoder.transformer.h.8.crossattention.masked_bias\", \"text_decoder.transformer.h.8.crossattention.c_attn.weight\", \"text_decoder.transformer.h.8.crossattention.c_attn.bias\", \"text_decoder.transformer.h.8.crossattention.q_attn.weight\", \"text_decoder.transformer.h.8.crossattention.q_attn.bias\", \"text_decoder.transformer.h.8.crossattention.c_proj.weight\", \"text_decoder.transformer.h.8.crossattention.c_proj.bias\", \"text_decoder.transformer.h.8.ln_cross_attn.weight\", \"text_decoder.transformer.h.8.ln_cross_attn.bias\", \"text_decoder.transformer.h.8.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.8.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.8.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.8.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.8.ln_2_crossattention.weight\", \"text_decoder.transformer.h.8.ln_2_crossattention.bias\", \"text_decoder.transformer.h.8.mlp.c_fc.weight\", \"text_decoder.transformer.h.8.mlp.c_fc.bias\", \"text_decoder.transformer.h.8.mlp.c_proj.weight\", \"text_decoder.transformer.h.8.mlp.c_proj.bias\", \"text_decoder.transformer.h.9.alpha_cattn\", \"text_decoder.transformer.h.9.alpha_dense\", \"text_decoder.transformer.h.9.ln_1.weight\", \"text_decoder.transformer.h.9.ln_1.bias\", \"text_decoder.transformer.h.9.attn.bias\", \"text_decoder.transformer.h.9.attn.masked_bias\", \"text_decoder.transformer.h.9.attn.c_attn.weight\", \"text_decoder.transformer.h.9.attn.c_attn.bias\", \"text_decoder.transformer.h.9.attn.c_proj.weight\", \"text_decoder.transformer.h.9.attn.c_proj.bias\", \"text_decoder.transformer.h.9.ln_2.weight\", \"text_decoder.transformer.h.9.ln_2.bias\", \"text_decoder.transformer.h.9.crossattention.bias\", \"text_decoder.transformer.h.9.crossattention.masked_bias\", \"text_decoder.transformer.h.9.crossattention.c_attn.weight\", \"text_decoder.transformer.h.9.crossattention.c_attn.bias\", \"text_decoder.transformer.h.9.crossattention.q_attn.weight\", \"text_decoder.transformer.h.9.crossattention.q_attn.bias\", \"text_decoder.transformer.h.9.crossattention.c_proj.weight\", \"text_decoder.transformer.h.9.crossattention.c_proj.bias\", \"text_decoder.transformer.h.9.ln_cross_attn.weight\", \"text_decoder.transformer.h.9.ln_cross_attn.bias\", \"text_decoder.transformer.h.9.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.9.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.9.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.9.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.9.ln_2_crossattention.weight\", \"text_decoder.transformer.h.9.ln_2_crossattention.bias\", \"text_decoder.transformer.h.9.mlp.c_fc.weight\", \"text_decoder.transformer.h.9.mlp.c_fc.bias\", \"text_decoder.transformer.h.9.mlp.c_proj.weight\", \"text_decoder.transformer.h.9.mlp.c_proj.bias\", \"text_decoder.transformer.h.10.alpha_cattn\", \"text_decoder.transformer.h.10.alpha_dense\", \"text_decoder.transformer.h.10.ln_1.weight\", \"text_decoder.transformer.h.10.ln_1.bias\", \"text_decoder.transformer.h.10.attn.bias\", \"text_decoder.transformer.h.10.attn.masked_bias\", \"text_decoder.transformer.h.10.attn.c_attn.weight\", \"text_decoder.transformer.h.10.attn.c_attn.bias\", \"text_decoder.transformer.h.10.attn.c_proj.weight\", \"text_decoder.transformer.h.10.attn.c_proj.bias\", \"text_decoder.transformer.h.10.ln_2.weight\", \"text_decoder.transformer.h.10.ln_2.bias\", \"text_decoder.transformer.h.10.crossattention.bias\", \"text_decoder.transformer.h.10.crossattention.masked_bias\", \"text_decoder.transformer.h.10.crossattention.c_attn.weight\", \"text_decoder.transformer.h.10.crossattention.c_attn.bias\", \"text_decoder.transformer.h.10.crossattention.q_attn.weight\", \"text_decoder.transformer.h.10.crossattention.q_attn.bias\", \"text_decoder.transformer.h.10.crossattention.c_proj.weight\", \"text_decoder.transformer.h.10.crossattention.c_proj.bias\", \"text_decoder.transformer.h.10.ln_cross_attn.weight\", \"text_decoder.transformer.h.10.ln_cross_attn.bias\", \"text_decoder.transformer.h.10.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.10.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.10.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.10.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.10.ln_2_crossattention.weight\", \"text_decoder.transformer.h.10.ln_2_crossattention.bias\", \"text_decoder.transformer.h.10.mlp.c_fc.weight\", \"text_decoder.transformer.h.10.mlp.c_fc.bias\", \"text_decoder.transformer.h.10.mlp.c_proj.weight\", \"text_decoder.transformer.h.10.mlp.c_proj.bias\", \"text_decoder.transformer.h.11.alpha_cattn\", \"text_decoder.transformer.h.11.alpha_dense\", \"text_decoder.transformer.h.11.ln_1.weight\", \"text_decoder.transformer.h.11.ln_1.bias\", \"text_decoder.transformer.h.11.attn.bias\", \"text_decoder.transformer.h.11.attn.masked_bias\", \"text_decoder.transformer.h.11.attn.c_attn.weight\", \"text_decoder.transformer.h.11.attn.c_attn.bias\", \"text_decoder.transformer.h.11.attn.c_proj.weight\", \"text_decoder.transformer.h.11.attn.c_proj.bias\", \"text_decoder.transformer.h.11.ln_2.weight\", \"text_decoder.transformer.h.11.ln_2.bias\", \"text_decoder.transformer.h.11.crossattention.bias\", \"text_decoder.transformer.h.11.crossattention.masked_bias\", \"text_decoder.transformer.h.11.crossattention.c_attn.weight\", \"text_decoder.transformer.h.11.crossattention.c_attn.bias\", \"text_decoder.transformer.h.11.crossattention.q_attn.weight\", \"text_decoder.transformer.h.11.crossattention.q_attn.bias\", \"text_decoder.transformer.h.11.crossattention.c_proj.weight\", \"text_decoder.transformer.h.11.crossattention.c_proj.bias\", \"text_decoder.transformer.h.11.ln_cross_attn.weight\", \"text_decoder.transformer.h.11.ln_cross_attn.bias\", \"text_decoder.transformer.h.11.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.11.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.11.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.11.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.11.ln_2_crossattention.weight\", \"text_decoder.transformer.h.11.ln_2_crossattention.bias\", \"text_decoder.transformer.h.11.mlp.c_fc.weight\", \"text_decoder.transformer.h.11.mlp.c_fc.bias\", \"text_decoder.transformer.h.11.mlp.c_proj.weight\", \"text_decoder.transformer.h.11.mlp.c_proj.bias\", \"text_decoder.transformer.ln_f.weight\", \"text_decoder.transformer.ln_f.bias\", \"text_decoder.lm_head.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m VideoRecap(old_args, eval_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m---> 22\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=> loaded resume checkpoint \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ckpt_path, ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[1;32m~\\.conda\\envs\\videorecap\\lib\\site-packages\\torch\\nn\\modules\\module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VideoRecap:\n\tUnexpected key(s) in state_dict: \"text_decoder.transformer.wte.weight\", \"text_decoder.transformer.wpe.weight\", \"text_decoder.transformer.h.0.alpha_cattn\", \"text_decoder.transformer.h.0.alpha_dense\", \"text_decoder.transformer.h.0.ln_1.weight\", \"text_decoder.transformer.h.0.ln_1.bias\", \"text_decoder.transformer.h.0.attn.bias\", \"text_decoder.transformer.h.0.attn.masked_bias\", \"text_decoder.transformer.h.0.attn.c_attn.weight\", \"text_decoder.transformer.h.0.attn.c_attn.bias\", \"text_decoder.transformer.h.0.attn.c_proj.weight\", \"text_decoder.transformer.h.0.attn.c_proj.bias\", \"text_decoder.transformer.h.0.ln_2.weight\", \"text_decoder.transformer.h.0.ln_2.bias\", \"text_decoder.transformer.h.0.crossattention.bias\", \"text_decoder.transformer.h.0.crossattention.masked_bias\", \"text_decoder.transformer.h.0.crossattention.c_attn.weight\", \"text_decoder.transformer.h.0.crossattention.c_attn.bias\", \"text_decoder.transformer.h.0.crossattention.q_attn.weight\", \"text_decoder.transformer.h.0.crossattention.q_attn.bias\", \"text_decoder.transformer.h.0.crossattention.c_proj.weight\", \"text_decoder.transformer.h.0.crossattention.c_proj.bias\", \"text_decoder.transformer.h.0.ln_cross_attn.weight\", \"text_decoder.transformer.h.0.ln_cross_attn.bias\", \"text_decoder.transformer.h.0.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.0.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.0.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.0.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.0.ln_2_crossattention.weight\", \"text_decoder.transformer.h.0.ln_2_crossattention.bias\", \"text_decoder.transformer.h.0.mlp.c_fc.weight\", \"text_decoder.transformer.h.0.mlp.c_fc.bias\", \"text_decoder.transformer.h.0.mlp.c_proj.weight\", \"text_decoder.transformer.h.0.mlp.c_proj.bias\", \"text_decoder.transformer.h.1.alpha_cattn\", \"text_decoder.transformer.h.1.alpha_dense\", \"text_decoder.transformer.h.1.ln_1.weight\", \"text_decoder.transformer.h.1.ln_1.bias\", \"text_decoder.transformer.h.1.attn.bias\", \"text_decoder.transformer.h.1.attn.masked_bias\", \"text_decoder.transformer.h.1.attn.c_attn.weight\", \"text_decoder.transformer.h.1.attn.c_attn.bias\", \"text_decoder.transformer.h.1.attn.c_proj.weight\", \"text_decoder.transformer.h.1.attn.c_proj.bias\", \"text_decoder.transformer.h.1.ln_2.weight\", \"text_decoder.transformer.h.1.ln_2.bias\", \"text_decoder.transformer.h.1.crossattention.bias\", \"text_decoder.transformer.h.1.crossattention.masked_bias\", \"text_decoder.transformer.h.1.crossattention.c_attn.weight\", \"text_decoder.transformer.h.1.crossattention.c_attn.bias\", \"text_decoder.transformer.h.1.crossattention.q_attn.weight\", \"text_decoder.transformer.h.1.crossattention.q_attn.bias\", \"text_decoder.transformer.h.1.crossattention.c_proj.weight\", \"text_decoder.transformer.h.1.crossattention.c_proj.bias\", \"text_decoder.transformer.h.1.ln_cross_attn.weight\", \"text_decoder.transformer.h.1.ln_cross_attn.bias\", \"text_decoder.transformer.h.1.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.1.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.1.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.1.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.1.ln_2_crossattention.weight\", \"text_decoder.transformer.h.1.ln_2_crossattention.bias\", \"text_decoder.transformer.h.1.mlp.c_fc.weight\", \"text_decoder.transformer.h.1.mlp.c_fc.bias\", \"text_decoder.transformer.h.1.mlp.c_proj.weight\", \"text_decoder.transformer.h.1.mlp.c_proj.bias\", \"text_decoder.transformer.h.2.alpha_cattn\", \"text_decoder.transformer.h.2.alpha_dense\", \"text_decoder.transformer.h.2.ln_1.weight\", \"text_decoder.transformer.h.2.ln_1.bias\", \"text_decoder.transformer.h.2.attn.bias\", \"text_decoder.transformer.h.2.attn.masked_bias\", \"text_decoder.transformer.h.2.attn.c_attn.weight\", \"text_decoder.transformer.h.2.attn.c_attn.bias\", \"text_decoder.transformer.h.2.attn.c_proj.weight\", \"text_decoder.transformer.h.2.attn.c_proj.bias\", \"text_decoder.transformer.h.2.ln_2.weight\", \"text_decoder.transformer.h.2.ln_2.bias\", \"text_decoder.transformer.h.2.crossattention.bias\", \"text_decoder.transformer.h.2.crossattention.masked_bias\", \"text_decoder.transformer.h.2.crossattention.c_attn.weight\", \"text_decoder.transformer.h.2.crossattention.c_attn.bias\", \"text_decoder.transformer.h.2.crossattention.q_attn.weight\", \"text_decoder.transformer.h.2.crossattention.q_attn.bias\", \"text_decoder.transformer.h.2.crossattention.c_proj.weight\", \"text_decoder.transformer.h.2.crossattention.c_proj.bias\", \"text_decoder.transformer.h.2.ln_cross_attn.weight\", \"text_decoder.transformer.h.2.ln_cross_attn.bias\", \"text_decoder.transformer.h.2.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.2.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.2.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.2.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.2.ln_2_crossattention.weight\", \"text_decoder.transformer.h.2.ln_2_crossattention.bias\", \"text_decoder.transformer.h.2.mlp.c_fc.weight\", \"text_decoder.transformer.h.2.mlp.c_fc.bias\", \"text_decoder.transformer.h.2.mlp.c_proj.weight\", \"text_decoder.transformer.h.2.mlp.c_proj.bias\", \"text_decoder.transformer.h.3.alpha_cattn\", \"text_decoder.transformer.h.3.alpha_dense\", \"text_decoder.transformer.h.3.ln_1.weight\", \"text_decoder.transformer.h.3.ln_1.bias\", \"text_decoder.transformer.h.3.attn.bias\", \"text_decoder.transformer.h.3.attn.masked_bias\", \"text_decoder.transformer.h.3.attn.c_attn.weight\", \"text_decoder.transformer.h.3.attn.c_attn.bias\", \"text_decoder.transformer.h.3.attn.c_proj.weight\", \"text_decoder.transformer.h.3.attn.c_proj.bias\", \"text_decoder.transformer.h.3.ln_2.weight\", \"text_decoder.transformer.h.3.ln_2.bias\", \"text_decoder.transformer.h.3.crossattention.bias\", \"text_decoder.transformer.h.3.crossattention.masked_bias\", \"text_decoder.transformer.h.3.crossattention.c_attn.weight\", \"text_decoder.transformer.h.3.crossattention.c_attn.bias\", \"text_decoder.transformer.h.3.crossattention.q_attn.weight\", \"text_decoder.transformer.h.3.crossattention.q_attn.bias\", \"text_decoder.transformer.h.3.crossattention.c_proj.weight\", \"text_decoder.transformer.h.3.crossattention.c_proj.bias\", \"text_decoder.transformer.h.3.ln_cross_attn.weight\", \"text_decoder.transformer.h.3.ln_cross_attn.bias\", \"text_decoder.transformer.h.3.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.3.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.3.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.3.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.3.ln_2_crossattention.weight\", \"text_decoder.transformer.h.3.ln_2_crossattention.bias\", \"text_decoder.transformer.h.3.mlp.c_fc.weight\", \"text_decoder.transformer.h.3.mlp.c_fc.bias\", \"text_decoder.transformer.h.3.mlp.c_proj.weight\", \"text_decoder.transformer.h.3.mlp.c_proj.bias\", \"text_decoder.transformer.h.4.alpha_cattn\", \"text_decoder.transformer.h.4.alpha_dense\", \"text_decoder.transformer.h.4.ln_1.weight\", \"text_decoder.transformer.h.4.ln_1.bias\", \"text_decoder.transformer.h.4.attn.bias\", \"text_decoder.transformer.h.4.attn.masked_bias\", \"text_decoder.transformer.h.4.attn.c_attn.weight\", \"text_decoder.transformer.h.4.attn.c_attn.bias\", \"text_decoder.transformer.h.4.attn.c_proj.weight\", \"text_decoder.transformer.h.4.attn.c_proj.bias\", \"text_decoder.transformer.h.4.ln_2.weight\", \"text_decoder.transformer.h.4.ln_2.bias\", \"text_decoder.transformer.h.4.crossattention.bias\", \"text_decoder.transformer.h.4.crossattention.masked_bias\", \"text_decoder.transformer.h.4.crossattention.c_attn.weight\", \"text_decoder.transformer.h.4.crossattention.c_attn.bias\", \"text_decoder.transformer.h.4.crossattention.q_attn.weight\", \"text_decoder.transformer.h.4.crossattention.q_attn.bias\", \"text_decoder.transformer.h.4.crossattention.c_proj.weight\", \"text_decoder.transformer.h.4.crossattention.c_proj.bias\", \"text_decoder.transformer.h.4.ln_cross_attn.weight\", \"text_decoder.transformer.h.4.ln_cross_attn.bias\", \"text_decoder.transformer.h.4.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.4.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.4.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.4.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.4.ln_2_crossattention.weight\", \"text_decoder.transformer.h.4.ln_2_crossattention.bias\", \"text_decoder.transformer.h.4.mlp.c_fc.weight\", \"text_decoder.transformer.h.4.mlp.c_fc.bias\", \"text_decoder.transformer.h.4.mlp.c_proj.weight\", \"text_decoder.transformer.h.4.mlp.c_proj.bias\", \"text_decoder.transformer.h.5.alpha_cattn\", \"text_decoder.transformer.h.5.alpha_dense\", \"text_decoder.transformer.h.5.ln_1.weight\", \"text_decoder.transformer.h.5.ln_1.bias\", \"text_decoder.transformer.h.5.attn.bias\", \"text_decoder.transformer.h.5.attn.masked_bias\", \"text_decoder.transformer.h.5.attn.c_attn.weight\", \"text_decoder.transformer.h.5.attn.c_attn.bias\", \"text_decoder.transformer.h.5.attn.c_proj.weight\", \"text_decoder.transformer.h.5.attn.c_proj.bias\", \"text_decoder.transformer.h.5.ln_2.weight\", \"text_decoder.transformer.h.5.ln_2.bias\", \"text_decoder.transformer.h.5.crossattention.bias\", \"text_decoder.transformer.h.5.crossattention.masked_bias\", \"text_decoder.transformer.h.5.crossattention.c_attn.weight\", \"text_decoder.transformer.h.5.crossattention.c_attn.bias\", \"text_decoder.transformer.h.5.crossattention.q_attn.weight\", \"text_decoder.transformer.h.5.crossattention.q_attn.bias\", \"text_decoder.transformer.h.5.crossattention.c_proj.weight\", \"text_decoder.transformer.h.5.crossattention.c_proj.bias\", \"text_decoder.transformer.h.5.ln_cross_attn.weight\", \"text_decoder.transformer.h.5.ln_cross_attn.bias\", \"text_decoder.transformer.h.5.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.5.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.5.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.5.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.5.ln_2_crossattention.weight\", \"text_decoder.transformer.h.5.ln_2_crossattention.bias\", \"text_decoder.transformer.h.5.mlp.c_fc.weight\", \"text_decoder.transformer.h.5.mlp.c_fc.bias\", \"text_decoder.transformer.h.5.mlp.c_proj.weight\", \"text_decoder.transformer.h.5.mlp.c_proj.bias\", \"text_decoder.transformer.h.6.alpha_cattn\", \"text_decoder.transformer.h.6.alpha_dense\", \"text_decoder.transformer.h.6.ln_1.weight\", \"text_decoder.transformer.h.6.ln_1.bias\", \"text_decoder.transformer.h.6.attn.bias\", \"text_decoder.transformer.h.6.attn.masked_bias\", \"text_decoder.transformer.h.6.attn.c_attn.weight\", \"text_decoder.transformer.h.6.attn.c_attn.bias\", \"text_decoder.transformer.h.6.attn.c_proj.weight\", \"text_decoder.transformer.h.6.attn.c_proj.bias\", \"text_decoder.transformer.h.6.ln_2.weight\", \"text_decoder.transformer.h.6.ln_2.bias\", \"text_decoder.transformer.h.6.crossattention.bias\", \"text_decoder.transformer.h.6.crossattention.masked_bias\", \"text_decoder.transformer.h.6.crossattention.c_attn.weight\", \"text_decoder.transformer.h.6.crossattention.c_attn.bias\", \"text_decoder.transformer.h.6.crossattention.q_attn.weight\", \"text_decoder.transformer.h.6.crossattention.q_attn.bias\", \"text_decoder.transformer.h.6.crossattention.c_proj.weight\", \"text_decoder.transformer.h.6.crossattention.c_proj.bias\", \"text_decoder.transformer.h.6.ln_cross_attn.weight\", \"text_decoder.transformer.h.6.ln_cross_attn.bias\", \"text_decoder.transformer.h.6.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.6.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.6.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.6.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.6.ln_2_crossattention.weight\", \"text_decoder.transformer.h.6.ln_2_crossattention.bias\", \"text_decoder.transformer.h.6.mlp.c_fc.weight\", \"text_decoder.transformer.h.6.mlp.c_fc.bias\", \"text_decoder.transformer.h.6.mlp.c_proj.weight\", \"text_decoder.transformer.h.6.mlp.c_proj.bias\", \"text_decoder.transformer.h.7.alpha_cattn\", \"text_decoder.transformer.h.7.alpha_dense\", \"text_decoder.transformer.h.7.ln_1.weight\", \"text_decoder.transformer.h.7.ln_1.bias\", \"text_decoder.transformer.h.7.attn.bias\", \"text_decoder.transformer.h.7.attn.masked_bias\", \"text_decoder.transformer.h.7.attn.c_attn.weight\", \"text_decoder.transformer.h.7.attn.c_attn.bias\", \"text_decoder.transformer.h.7.attn.c_proj.weight\", \"text_decoder.transformer.h.7.attn.c_proj.bias\", \"text_decoder.transformer.h.7.ln_2.weight\", \"text_decoder.transformer.h.7.ln_2.bias\", \"text_decoder.transformer.h.7.crossattention.bias\", \"text_decoder.transformer.h.7.crossattention.masked_bias\", \"text_decoder.transformer.h.7.crossattention.c_attn.weight\", \"text_decoder.transformer.h.7.crossattention.c_attn.bias\", \"text_decoder.transformer.h.7.crossattention.q_attn.weight\", \"text_decoder.transformer.h.7.crossattention.q_attn.bias\", \"text_decoder.transformer.h.7.crossattention.c_proj.weight\", \"text_decoder.transformer.h.7.crossattention.c_proj.bias\", \"text_decoder.transformer.h.7.ln_cross_attn.weight\", \"text_decoder.transformer.h.7.ln_cross_attn.bias\", \"text_decoder.transformer.h.7.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.7.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.7.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.7.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.7.ln_2_crossattention.weight\", \"text_decoder.transformer.h.7.ln_2_crossattention.bias\", \"text_decoder.transformer.h.7.mlp.c_fc.weight\", \"text_decoder.transformer.h.7.mlp.c_fc.bias\", \"text_decoder.transformer.h.7.mlp.c_proj.weight\", \"text_decoder.transformer.h.7.mlp.c_proj.bias\", \"text_decoder.transformer.h.8.alpha_cattn\", \"text_decoder.transformer.h.8.alpha_dense\", \"text_decoder.transformer.h.8.ln_1.weight\", \"text_decoder.transformer.h.8.ln_1.bias\", \"text_decoder.transformer.h.8.attn.bias\", \"text_decoder.transformer.h.8.attn.masked_bias\", \"text_decoder.transformer.h.8.attn.c_attn.weight\", \"text_decoder.transformer.h.8.attn.c_attn.bias\", \"text_decoder.transformer.h.8.attn.c_proj.weight\", \"text_decoder.transformer.h.8.attn.c_proj.bias\", \"text_decoder.transformer.h.8.ln_2.weight\", \"text_decoder.transformer.h.8.ln_2.bias\", \"text_decoder.transformer.h.8.crossattention.bias\", \"text_decoder.transformer.h.8.crossattention.masked_bias\", \"text_decoder.transformer.h.8.crossattention.c_attn.weight\", \"text_decoder.transformer.h.8.crossattention.c_attn.bias\", \"text_decoder.transformer.h.8.crossattention.q_attn.weight\", \"text_decoder.transformer.h.8.crossattention.q_attn.bias\", \"text_decoder.transformer.h.8.crossattention.c_proj.weight\", \"text_decoder.transformer.h.8.crossattention.c_proj.bias\", \"text_decoder.transformer.h.8.ln_cross_attn.weight\", \"text_decoder.transformer.h.8.ln_cross_attn.bias\", \"text_decoder.transformer.h.8.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.8.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.8.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.8.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.8.ln_2_crossattention.weight\", \"text_decoder.transformer.h.8.ln_2_crossattention.bias\", \"text_decoder.transformer.h.8.mlp.c_fc.weight\", \"text_decoder.transformer.h.8.mlp.c_fc.bias\", \"text_decoder.transformer.h.8.mlp.c_proj.weight\", \"text_decoder.transformer.h.8.mlp.c_proj.bias\", \"text_decoder.transformer.h.9.alpha_cattn\", \"text_decoder.transformer.h.9.alpha_dense\", \"text_decoder.transformer.h.9.ln_1.weight\", \"text_decoder.transformer.h.9.ln_1.bias\", \"text_decoder.transformer.h.9.attn.bias\", \"text_decoder.transformer.h.9.attn.masked_bias\", \"text_decoder.transformer.h.9.attn.c_attn.weight\", \"text_decoder.transformer.h.9.attn.c_attn.bias\", \"text_decoder.transformer.h.9.attn.c_proj.weight\", \"text_decoder.transformer.h.9.attn.c_proj.bias\", \"text_decoder.transformer.h.9.ln_2.weight\", \"text_decoder.transformer.h.9.ln_2.bias\", \"text_decoder.transformer.h.9.crossattention.bias\", \"text_decoder.transformer.h.9.crossattention.masked_bias\", \"text_decoder.transformer.h.9.crossattention.c_attn.weight\", \"text_decoder.transformer.h.9.crossattention.c_attn.bias\", \"text_decoder.transformer.h.9.crossattention.q_attn.weight\", \"text_decoder.transformer.h.9.crossattention.q_attn.bias\", \"text_decoder.transformer.h.9.crossattention.c_proj.weight\", \"text_decoder.transformer.h.9.crossattention.c_proj.bias\", \"text_decoder.transformer.h.9.ln_cross_attn.weight\", \"text_decoder.transformer.h.9.ln_cross_attn.bias\", \"text_decoder.transformer.h.9.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.9.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.9.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.9.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.9.ln_2_crossattention.weight\", \"text_decoder.transformer.h.9.ln_2_crossattention.bias\", \"text_decoder.transformer.h.9.mlp.c_fc.weight\", \"text_decoder.transformer.h.9.mlp.c_fc.bias\", \"text_decoder.transformer.h.9.mlp.c_proj.weight\", \"text_decoder.transformer.h.9.mlp.c_proj.bias\", \"text_decoder.transformer.h.10.alpha_cattn\", \"text_decoder.transformer.h.10.alpha_dense\", \"text_decoder.transformer.h.10.ln_1.weight\", \"text_decoder.transformer.h.10.ln_1.bias\", \"text_decoder.transformer.h.10.attn.bias\", \"text_decoder.transformer.h.10.attn.masked_bias\", \"text_decoder.transformer.h.10.attn.c_attn.weight\", \"text_decoder.transformer.h.10.attn.c_attn.bias\", \"text_decoder.transformer.h.10.attn.c_proj.weight\", \"text_decoder.transformer.h.10.attn.c_proj.bias\", \"text_decoder.transformer.h.10.ln_2.weight\", \"text_decoder.transformer.h.10.ln_2.bias\", \"text_decoder.transformer.h.10.crossattention.bias\", \"text_decoder.transformer.h.10.crossattention.masked_bias\", \"text_decoder.transformer.h.10.crossattention.c_attn.weight\", \"text_decoder.transformer.h.10.crossattention.c_attn.bias\", \"text_decoder.transformer.h.10.crossattention.q_attn.weight\", \"text_decoder.transformer.h.10.crossattention.q_attn.bias\", \"text_decoder.transformer.h.10.crossattention.c_proj.weight\", \"text_decoder.transformer.h.10.crossattention.c_proj.bias\", \"text_decoder.transformer.h.10.ln_cross_attn.weight\", \"text_decoder.transformer.h.10.ln_cross_attn.bias\", \"text_decoder.transformer.h.10.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.10.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.10.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.10.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.10.ln_2_crossattention.weight\", \"text_decoder.transformer.h.10.ln_2_crossattention.bias\", \"text_decoder.transformer.h.10.mlp.c_fc.weight\", \"text_decoder.transformer.h.10.mlp.c_fc.bias\", \"text_decoder.transformer.h.10.mlp.c_proj.weight\", \"text_decoder.transformer.h.10.mlp.c_proj.bias\", \"text_decoder.transformer.h.11.alpha_cattn\", \"text_decoder.transformer.h.11.alpha_dense\", \"text_decoder.transformer.h.11.ln_1.weight\", \"text_decoder.transformer.h.11.ln_1.bias\", \"text_decoder.transformer.h.11.attn.bias\", \"text_decoder.transformer.h.11.attn.masked_bias\", \"text_decoder.transformer.h.11.attn.c_attn.weight\", \"text_decoder.transformer.h.11.attn.c_attn.bias\", \"text_decoder.transformer.h.11.attn.c_proj.weight\", \"text_decoder.transformer.h.11.attn.c_proj.bias\", \"text_decoder.transformer.h.11.ln_2.weight\", \"text_decoder.transformer.h.11.ln_2.bias\", \"text_decoder.transformer.h.11.crossattention.bias\", \"text_decoder.transformer.h.11.crossattention.masked_bias\", \"text_decoder.transformer.h.11.crossattention.c_attn.weight\", \"text_decoder.transformer.h.11.crossattention.c_attn.bias\", \"text_decoder.transformer.h.11.crossattention.q_attn.weight\", \"text_decoder.transformer.h.11.crossattention.q_attn.bias\", \"text_decoder.transformer.h.11.crossattention.c_proj.weight\", \"text_decoder.transformer.h.11.crossattention.c_proj.bias\", \"text_decoder.transformer.h.11.ln_cross_attn.weight\", \"text_decoder.transformer.h.11.ln_cross_attn.bias\", \"text_decoder.transformer.h.11.mlp_crossattention.c_fc.weight\", \"text_decoder.transformer.h.11.mlp_crossattention.c_fc.bias\", \"text_decoder.transformer.h.11.mlp_crossattention.c_proj.weight\", \"text_decoder.transformer.h.11.mlp_crossattention.c_proj.bias\", \"text_decoder.transformer.h.11.ln_2_crossattention.weight\", \"text_decoder.transformer.h.11.ln_2_crossattention.bias\", \"text_decoder.transformer.h.11.mlp.c_fc.weight\", \"text_decoder.transformer.h.11.mlp.c_fc.bias\", \"text_decoder.transformer.h.11.mlp.c_proj.weight\", \"text_decoder.transformer.h.11.mlp.c_proj.bias\", \"text_decoder.transformer.ln_f.weight\", \"text_decoder.transformer.ln_f.bias\", \"text_decoder.lm_head.weight\". "
     ]
    }
   ],
   "source": [
    "# Create model and tokenizer\n",
    "ckpt_path = 'pretrained_models/videorecap/videorecap_clip.pt'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "old_args = ckpt['args']\n",
    "old_args.video_feature_type = 'pixel'  \n",
    "old_args.num_video_feat=4                     # number of frames per clip caption\n",
    "crop_size = 224\n",
    "transform = transforms.Compose([\n",
    "        Permute([3, 0, 1, 2]),  # T H W C -> C T H W\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms_video.NormalizeVideo(mean=[108.3272985, 116.7460125, 104.09373615000001], std=[68.5005327, 66.6321579, 70.32316305]),\n",
    "    ])\n",
    "tokenizer = AutoTokenizer.from_pretrained(old_args.decoder_name)\n",
    "state_dict = OrderedDict()\n",
    "for k, v in ckpt['state_dict'].items():\n",
    "    state_dict[k.replace('module.', '')] = v\n",
    "\n",
    "print(\"=> Creating model\")\n",
    "model = VideoRecap(old_args, eval_only=True)\n",
    "model = model.cuda()\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "print(\"=> loaded resume checkpoint '{}' (epoch {})\".format(ckpt_path, ckpt['epoch']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad0c3a15-d56a-40fa-83dc-d64c5e879eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video length 147.8 seconds\n",
      "number of captions 37\n",
      "[['clientVid1', 0.0, 4.0], ['clientVid1', 4.0, 8.0], ['clientVid1', 8.0, 12.0], ['clientVid1', 12.0, 16.0], ['clientVid1', 16.0, 20.0], ['clientVid1', 20.0, 24.0], ['clientVid1', 24.0, 28.0], ['clientVid1', 28.0, 32.0], ['clientVid1', 32.0, 36.0], ['clientVid1', 36.0, 40.0], ['clientVid1', 40.0, 44.0], ['clientVid1', 44.0, 48.0], ['clientVid1', 48.0, 52.0], ['clientVid1', 52.0, 56.0], ['clientVid1', 56.0, 60.0], ['clientVid1', 60.0, 64.0], ['clientVid1', 64.0, 68.0], ['clientVid1', 68.0, 72.0], ['clientVid1', 72.0, 76.0], ['clientVid1', 76.0, 80.0], ['clientVid1', 80.0, 84.0], ['clientVid1', 84.0, 88.0], ['clientVid1', 88.0, 92.0], ['clientVid1', 92.0, 96.0], ['clientVid1', 96.0, 100.0], ['clientVid1', 100.0, 104.0], ['clientVid1', 104.0, 108.0], ['clientVid1', 108.0, 112.0], ['clientVid1', 112.0, 116.0], ['clientVid1', 116.0, 120.0], ['clientVid1', 120.0, 124.0], ['clientVid1', 124.0, 128.0], ['clientVid1', 128.0, 132.0], ['clientVid1', 132.0, 136.0], ['clientVid1', 136.0, 140.0], ['clientVid1', 140.0, 144.0], ['clientVid1', 144.0, 147.8]]\n",
      "37 5\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001850A911DF0>\n"
     ]
    }
   ],
   "source": [
    "# Create dataset from the video\n",
    "video = VideoFileClip(video_file)\n",
    "print('Video length', video.duration, 'seconds')\n",
    "\n",
    "video_length = video.duration\n",
    "caption_duration = 4                              # Extract clip caption at each 4 seconds\n",
    "old_args.video_loader_type='moviepy'\n",
    "old_args.chunk_len = -1                           # load from raw video\n",
    "old_args.video_feature_path = 'assets'            # path to the video folder \n",
    "metadata = []  \n",
    "for i in np.arange(0, video_length, caption_duration):\n",
    "    metadata.append([vid, i, min(i + caption_duration, video_length)])    # video name is example.mp4 so assuming video id=example\n",
    "print('number of captions', len(metadata))\n",
    "print(metadata)\n",
    "\n",
    "old_args.metadata = metadata\n",
    "dataset = VideoCaptionDataset(old_args, transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=False, \n",
    "                                        num_workers=8, pin_memory=True, drop_last=False)\n",
    "print(len(dataset), len(data_loader))\n",
    "print(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f77eccd-d39f-4602-b352-47c0f22ceb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57ea7bff-ae47-42fc-8960-55d42ca2dae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "int64\n",
      "tensor([[[ 0.5538, -0.5771, -0.7793,  ..., -0.2779,  0.5573,  0.5015],\n",
      "         [ 0.5554,  1.6145,  0.8115,  ..., -0.3208,  0.2458, -0.8327],\n",
      "         [ 0.4421,  1.5578,  0.6258,  ..., -0.3875,  0.2385, -0.8783],\n",
      "         ...,\n",
      "         [-0.2527, -0.7677,  0.3102,  ...,  1.1884, -0.8021,  0.2423],\n",
      "         [ 0.7850, -0.9263,  0.3181,  ..., -0.0571, -1.4131, -0.7242],\n",
      "         [ 1.0209, -1.3234,  0.0450,  ..., -0.0305, -1.5404, -0.5811]],\n",
      "\n",
      "        [[-0.8042,  1.1468,  0.3420,  ..., -0.4703,  0.0082,  0.3745],\n",
      "         [ 0.2162,  1.5118, -0.1716,  ..., -0.4051,  1.4034,  0.1122],\n",
      "         [ 0.1700,  1.1187,  0.1205,  ..., -0.3028,  2.1208,  0.9864],\n",
      "         ...,\n",
      "         [ 0.2334, -0.1190,  0.6973,  ...,  0.8849, -0.0576, -0.9527],\n",
      "         [-0.3140,  0.1205,  1.0912,  ..., -0.2629, -0.5567, -0.9601],\n",
      "         [ 0.1506,  0.0774,  0.9076,  ...,  0.4581, -0.0771, -1.0298]],\n",
      "\n",
      "        [[-0.5454, -0.1095, -0.4433,  ...,  0.2002,  1.0965, -0.1942],\n",
      "         [ 0.5511,  0.3666,  0.6140,  ...,  0.4373,  0.6308,  0.0295],\n",
      "         [ 0.0945,  0.2296,  0.2857,  ...,  0.2464,  0.6664, -0.3222],\n",
      "         ...,\n",
      "         [ 0.4621,  0.4004, -0.6691,  ...,  1.0666,  0.7946, -0.4173],\n",
      "         [-0.0498,  0.1487, -0.0498,  ...,  0.1214,  0.7056, -0.4465],\n",
      "         [ 0.4641,  0.3025, -0.4591,  ...,  1.1404,  0.8426, -1.0622]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0727, -0.5406, -0.5501,  ..., -0.9579, -0.0940, -0.7150],\n",
      "         [-0.2366,  1.1757, -0.3117,  ..., -0.5158,  0.3406, -0.0644],\n",
      "         [ 0.2082,  0.9509,  0.3589,  ..., -0.7459,  0.3078, -0.3487],\n",
      "         ...,\n",
      "         [-0.1080, -0.1803, -0.3997,  ..., -0.1800, -0.5244,  0.1536],\n",
      "         [-0.4437, -0.5676,  0.0627,  ...,  0.2022, -0.0841, -0.2209],\n",
      "         [-0.2450, -0.7510,  0.1177,  ...,  0.2524,  0.0563, -0.5383]],\n",
      "\n",
      "        [[ 0.0872,  0.6445, -0.0976,  ...,  0.9995,  0.9092, -0.0408],\n",
      "         [ 0.4727,  0.2477, -0.2778,  ...,  0.2634,  0.6373, -0.0411],\n",
      "         [ 0.3700,  0.9312, -0.1345,  ...,  0.3197,  0.3160,  0.1751],\n",
      "         ...,\n",
      "         [-1.3458, -0.5116,  0.6871,  ...,  1.0371,  0.1825, -0.8506],\n",
      "         [ 0.4714, -0.3392, -0.0699,  ...,  0.3181,  0.8432,  0.4089],\n",
      "         [-0.2521, -0.0962, -0.2160,  ...,  0.8516,  0.1463, -1.6511]],\n",
      "\n",
      "        [[ 0.1788,  0.3105,  0.1305,  ...,  1.2059, -0.1102, -0.8419],\n",
      "         [-0.7896,  0.1480, -1.0590,  ...,  0.3249,  1.7359, -0.9940],\n",
      "         [-0.5479, -0.5585, -0.3126,  ...,  0.0118,  0.8439, -0.6579],\n",
      "         ...,\n",
      "         [ 0.0850,  0.1625, -0.0480,  ...,  0.7779,  2.0792,  0.0068],\n",
      "         [ 0.5535,  0.1360,  0.7251,  ..., -0.3689,  1.2946,  0.7567],\n",
      "         [-0.0882, -0.2099,  0.1789,  ..., -0.5147, -0.1674,  0.5203]]],\n",
      "       device='cuda:0')\n",
      "\n",
      "#C C walks around the building\n",
      "#C C adjusts the camera\n",
      "#C C opens the door\n",
      "#C C passes the cup to the woman X with her right hand\n",
      "#C C drinks tea\n",
      "#C C walks around\n",
      "#C C picks a boot\n",
      "#C C rides the bicycle\n",
      "tensor([ 8,  9, 10, 11, 12, 13, 14, 15])\n",
      "int64\n",
      "tensor([[[ 2.1568e-01,  6.6973e-01,  1.8415e-01,  ...,  9.8880e-01,\n",
      "          -4.6834e-01, -7.4666e-01],\n",
      "         [-4.5626e-01,  3.8619e-01,  7.4413e-01,  ..., -4.4940e-01,\n",
      "           3.6274e-01,  8.9045e-01],\n",
      "         [-4.5130e-01,  5.7706e-01,  6.7055e-01,  ..., -6.2006e-01,\n",
      "           6.6661e-01,  1.1938e+00],\n",
      "         ...,\n",
      "         [-1.3219e+00,  6.3532e-02,  7.5311e-01,  ..., -2.6920e-01,\n",
      "          -1.0486e+00, -6.1299e-01],\n",
      "         [-7.0011e-01,  3.7043e-01,  4.9533e-02,  ..., -2.3207e-01,\n",
      "          -7.7542e-01, -1.0124e+00],\n",
      "         [ 1.1454e-01, -5.4516e-01,  3.7551e-01,  ...,  1.0382e+00,\n",
      "          -9.5065e-01, -1.5253e+00]],\n",
      "\n",
      "        [[ 1.6074e-01,  1.9213e-01, -1.3358e+00,  ...,  3.4652e-01,\n",
      "          -6.4475e-01,  1.2391e+00],\n",
      "         [ 1.1514e+00,  6.2325e-01, -9.7241e-01,  ..., -6.4054e-01,\n",
      "           6.2035e-01,  3.0037e-01],\n",
      "         [ 3.6787e-01,  4.7585e-01, -6.2672e-01,  ..., -4.3079e-01,\n",
      "           7.0614e-01,  1.0825e-01],\n",
      "         ...,\n",
      "         [-5.1444e-01,  8.6756e-01,  6.9223e-01,  ...,  6.1162e-01,\n",
      "          -4.0440e-02, -2.0781e-01],\n",
      "         [-3.4539e-03,  1.4569e+00,  3.5719e-01,  ..., -3.6721e-02,\n",
      "           1.6032e+00,  1.0475e-02],\n",
      "         [ 3.2197e-01,  1.3771e+00,  1.3954e-01,  ...,  5.2525e-01,\n",
      "           1.2942e+00,  7.1392e-01]],\n",
      "\n",
      "        [[ 6.1261e-01,  9.1317e-01, -9.6812e-01,  ...,  6.3883e-01,\n",
      "           3.8243e-01,  3.3161e-01],\n",
      "         [ 3.3395e-01,  6.9268e-01,  8.3801e-02,  ...,  1.4364e-01,\n",
      "           9.6430e-01, -1.6082e+00],\n",
      "         [ 4.6662e-01,  4.3205e-01, -6.1685e-01,  ...,  4.3180e-01,\n",
      "           1.8960e+00, -8.8089e-01],\n",
      "         ...,\n",
      "         [ 2.9634e-03,  5.1755e-01, -4.2456e-01,  ..., -7.7520e-02,\n",
      "          -7.0830e-02,  5.0705e-01],\n",
      "         [ 5.9800e-02, -3.3307e-01, -2.3768e-01,  ..., -1.6540e-01,\n",
      "          -2.2314e-02,  2.2184e-01],\n",
      "         [-7.9659e-01, -4.5346e-01,  3.4944e-01,  ..., -8.6812e-02,\n",
      "           3.8922e-01, -9.6973e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.7718e-01,  1.1273e-01, -9.3835e-03,  ..., -1.3412e+00,\n",
      "          -1.0389e+00, -3.1841e-01],\n",
      "         [ 4.0248e-01,  8.7634e-01,  9.0595e-01,  ...,  2.5647e-02,\n",
      "           2.9268e-01, -3.7215e-01],\n",
      "         [-4.4836e-01,  8.2568e-01,  7.5719e-01,  ...,  3.5915e-01,\n",
      "           1.8555e-01, -1.0060e+00],\n",
      "         ...,\n",
      "         [-2.1546e-01,  4.2770e-01,  4.7537e-01,  ..., -6.9069e-01,\n",
      "           4.6029e-01,  1.3296e+00],\n",
      "         [ 6.0534e-01,  4.6441e-01,  7.4640e-01,  ..., -7.7974e-01,\n",
      "           1.2185e-02,  1.6594e+00],\n",
      "         [ 2.9807e-01, -7.2207e-02, -8.2880e-02,  ...,  1.9025e-01,\n",
      "           4.3483e-01,  2.5469e-01]],\n",
      "\n",
      "        [[ 6.0396e-01,  7.1486e-01, -7.7901e-01,  ..., -1.8029e-01,\n",
      "           1.0727e+00,  8.7072e-01],\n",
      "         [-7.2040e-01,  6.8844e-01,  3.6592e-01,  ...,  1.3488e+00,\n",
      "          -2.9234e-01,  6.5914e-01],\n",
      "         [ 6.0771e-01,  4.9744e-01, -2.2318e-01,  ...,  1.1925e-01,\n",
      "          -2.7412e-02,  1.3591e-01],\n",
      "         ...,\n",
      "         [-3.2476e-01,  1.2242e-01, -5.2641e-04,  ...,  3.1663e-01,\n",
      "           9.1155e-01, -3.0284e-01],\n",
      "         [ 2.9925e-01, -3.5959e-01, -5.3747e-01,  ...,  3.5669e-01,\n",
      "           2.0295e-01,  4.3520e-01],\n",
      "         [ 9.6514e-02, -2.3637e-01,  8.1831e-01,  ...,  7.8232e-01,\n",
      "           1.6405e+00, -1.0358e+00]],\n",
      "\n",
      "        [[-5.4831e-01,  9.0742e-01, -5.5013e-02,  ...,  8.4221e-01,\n",
      "          -1.0925e+00,  1.2614e+00],\n",
      "         [-8.7843e-01, -3.3277e-01, -3.4850e-01,  ...,  3.9997e-01,\n",
      "           5.6757e-01,  5.0390e-01],\n",
      "         [-8.5766e-01,  2.4150e-01,  6.0194e-01,  ..., -3.6297e-01,\n",
      "           1.5214e+00, -4.6024e-02],\n",
      "         ...,\n",
      "         [-3.8722e-01,  5.9433e-01,  4.6405e-02,  ...,  3.6441e-02,\n",
      "           4.1246e-02,  9.9283e-01],\n",
      "         [-5.5478e-01,  3.4947e-02,  2.1694e-01,  ...,  6.6821e-01,\n",
      "          -1.3612e-01, -2.9110e-01],\n",
      "         [ 1.3077e-01,  3.7412e-01,  2.1188e-01,  ...,  1.2857e-01,\n",
      "           2.1000e-01,  9.9267e-01]]], device='cuda:0')\n",
      "\n",
      "#C C looks around\n",
      "#C C looks around\n",
      "#C C looks around\n",
      "#C C rides the bicycle\n",
      "#C C points at the road\n",
      "#C C points at the phone\n",
      "#C C passes the paper to his right hand\n",
      "#C C looks around\n",
      "tensor([16, 17, 18, 19, 20, 21, 22, 23])\n",
      "int64\n",
      "tensor([[[ 1.7816e-01, -6.1611e-01,  1.6095e-01,  ..., -7.9421e-01,\n",
      "          -3.4638e-01, -3.4570e-01],\n",
      "         [ 2.4388e-01, -9.0490e-01,  7.7732e-01,  ..., -1.5167e-02,\n",
      "           2.1410e-01, -3.8015e-01],\n",
      "         [ 1.0460e+00, -4.4825e-01,  3.2261e-01,  ..., -6.6555e-01,\n",
      "           3.1373e-01, -3.2277e-01],\n",
      "         ...,\n",
      "         [ 8.3220e-01,  7.9480e-01, -4.8613e-01,  ..., -1.7267e+00,\n",
      "           6.9638e-01,  3.8319e-02],\n",
      "         [-1.3195e+00, -4.8958e-01,  1.1242e-01,  ..., -2.8542e-01,\n",
      "           9.4662e-01,  2.5720e-01],\n",
      "         [-1.8157e+00, -6.4272e-01, -1.6550e-01,  ...,  5.7729e-01,\n",
      "           2.5593e-01, -9.4330e-01]],\n",
      "\n",
      "        [[ 6.0786e-01, -8.3551e-01,  4.4877e-01,  ..., -6.7403e-01,\n",
      "          -2.4469e-01,  1.6251e-01],\n",
      "         [-6.0386e-01, -1.0972e+00,  1.1138e+00,  ...,  1.4289e-01,\n",
      "           9.0286e-01, -3.9852e-01],\n",
      "         [-6.1412e-01, -3.8901e-02,  5.6560e-01,  ..., -1.6311e-01,\n",
      "           4.8871e-01, -4.1097e-01],\n",
      "         ...,\n",
      "         [ 6.0052e-01, -9.6871e-03,  8.7766e-01,  ..., -3.9948e-01,\n",
      "           9.5369e-01, -2.3436e-01],\n",
      "         [ 3.2850e-01,  3.6569e-02,  9.5390e-01,  ..., -3.6125e-01,\n",
      "           7.6638e-01, -4.5185e-01],\n",
      "         [-2.4716e-01, -5.3089e-01,  7.7571e-01,  ..., -7.3935e-02,\n",
      "           7.4211e-01, -2.3185e-01]],\n",
      "\n",
      "        [[-4.0008e-01, -6.3491e-01, -3.0528e-01,  ..., -2.1073e+00,\n",
      "           2.7691e-01,  1.1751e+00],\n",
      "         [-5.5991e-01, -1.5023e-01, -2.9242e-01,  ...,  1.3006e+00,\n",
      "          -5.4980e-01,  1.5752e-01],\n",
      "         [-2.8254e-01,  2.5128e-01,  7.8252e-02,  ...,  2.6773e-01,\n",
      "          -5.9644e-01,  1.4228e-01],\n",
      "         ...,\n",
      "         [ 1.7937e-01, -8.8716e-01, -3.6188e-02,  ..., -8.2368e-02,\n",
      "           3.8424e-01, -4.7127e-01],\n",
      "         [-2.1484e-01, -1.0700e+00,  8.2463e-02,  ..., -3.3066e-01,\n",
      "           6.7505e-01, -2.1592e-01],\n",
      "         [-4.5504e-01, -4.3553e-01,  4.5408e-01,  ..., -7.4930e-01,\n",
      "           1.1841e+00, -2.6311e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.9164e-01, -1.2081e+00, -8.6979e-01,  ..., -3.5192e-01,\n",
      "          -1.6332e+00, -2.0489e-01],\n",
      "         [-4.2655e-01,  1.1643e-02, -5.5831e-01,  ..., -6.1440e-01,\n",
      "           1.1764e+00,  1.2955e-01],\n",
      "         [-4.2642e-01, -1.2390e-01, -5.3799e-01,  ..., -3.2339e-01,\n",
      "           1.1653e+00, -1.9849e-02],\n",
      "         ...,\n",
      "         [ 1.5826e-01,  1.2140e-01,  1.2096e+00,  ...,  6.9970e-02,\n",
      "           5.0018e-01, -3.7491e-01],\n",
      "         [ 1.4626e-01, -5.2197e-01,  5.7914e-01,  ...,  1.5279e-01,\n",
      "           2.9394e-01, -4.0371e-01],\n",
      "         [ 1.2907e-02, -7.7054e-01,  7.5681e-01,  ...,  4.8563e-01,\n",
      "           2.7757e-01, -8.6255e-01]],\n",
      "\n",
      "        [[ 1.2404e+00,  1.1996e-01, -3.5733e-01,  ...,  2.4918e-01,\n",
      "           7.7051e-01,  1.9133e+00],\n",
      "         [-1.9893e-01,  3.0944e-01, -1.2374e+00,  ..., -6.9386e-04,\n",
      "           1.3938e+00,  7.4050e-01],\n",
      "         [-4.8411e-02,  7.5555e-01, -9.6502e-01,  ..., -7.1452e-01,\n",
      "           7.4166e-01,  1.7274e-01],\n",
      "         ...,\n",
      "         [-2.4042e-02,  7.7775e-01,  9.3092e-01,  ...,  1.1088e+00,\n",
      "          -1.1780e+00, -1.3685e-01],\n",
      "         [ 6.1808e-01, -4.7515e-01,  1.4389e+00,  ...,  2.0391e-01,\n",
      "           3.5789e-01,  1.4227e-01],\n",
      "         [-2.4335e-02, -5.0920e-01,  3.2340e-01,  ...,  2.7892e-01,\n",
      "           2.9678e-01, -2.5552e-01]],\n",
      "\n",
      "        [[-2.1600e-01,  5.4969e-02, -7.8688e-01,  ..., -5.8596e-01,\n",
      "          -6.5248e-01,  9.9199e-02],\n",
      "         [-1.5202e-01,  4.9771e-01, -9.6458e-01,  ...,  2.9014e-03,\n",
      "           5.6607e-01,  1.2340e+00],\n",
      "         [-1.1764e-01,  1.3185e-01, -1.7952e-01,  ..., -7.4447e-02,\n",
      "          -6.6767e-01,  8.5011e-01],\n",
      "         ...,\n",
      "         [ 2.6867e-01,  1.4490e+00, -8.7300e-01,  ...,  6.6602e-01,\n",
      "          -6.5348e-01,  9.2388e-01],\n",
      "         [ 6.5478e-01,  1.0014e+00, -9.0524e-01,  ..., -9.7354e-02,\n",
      "          -4.2890e-01,  7.3863e-01],\n",
      "         [-4.9269e-01,  9.1690e-01, -6.0650e-01,  ...,  1.6735e-01,\n",
      "           6.2192e-01,  8.3144e-01]]], device='cuda:0')\n",
      "\n",
      "#C C looks around\n",
      "#C C looks at the plant\n",
      "#C C climbs the stairs\n",
      "#C C takes a picture with the phone in both hands\n",
      "#C C looks around the field\n",
      "#C C looks around\n",
      "#C C looks at the lake\n",
      "#C C looks around\n",
      "tensor([24, 25, 26, 27, 28, 29, 30, 31])\n",
      "int64\n",
      "tensor([[[-1.4142e-01, -4.6690e-01, -1.0885e+00,  ...,  3.7107e-01,\n",
      "           2.1169e-02,  9.9796e-01],\n",
      "         [ 3.4163e-01,  3.4842e-01, -1.9430e+00,  ...,  2.4734e-01,\n",
      "           2.8849e-01,  1.2696e+00],\n",
      "         [ 5.1653e-01,  1.1284e+00, -2.1616e+00,  ..., -6.0086e-01,\n",
      "           2.0773e-01,  6.4823e-01],\n",
      "         ...,\n",
      "         [-3.9803e-01,  1.0065e+00, -1.3839e-01,  ...,  4.0050e-01,\n",
      "           1.3765e-02, -1.7105e-01],\n",
      "         [ 2.0069e-01,  2.0561e+00,  5.3648e-02,  ..., -4.3506e-01,\n",
      "          -5.3251e-02, -3.8617e-01],\n",
      "         [-5.9178e-01, -2.1491e-01,  6.6628e-01,  ...,  5.8207e-01,\n",
      "           9.9863e-01, -6.8442e-01]],\n",
      "\n",
      "        [[-6.2296e-01,  3.5963e-01, -6.4120e-01,  ..., -2.1324e-01,\n",
      "           4.3041e-01,  1.1474e+00],\n",
      "         [ 1.5592e-01,  5.3180e-01,  2.4757e-01,  ..., -8.5326e-01,\n",
      "           5.9533e-01,  4.8453e-01],\n",
      "         [-1.7051e-01,  1.9855e-01,  2.4754e-01,  ..., -2.0988e-01,\n",
      "           5.6983e-01,  3.2044e-01],\n",
      "         ...,\n",
      "         [ 1.3428e-01,  3.5628e-01,  3.6963e-01,  ..., -1.2096e-01,\n",
      "           1.9543e+00, -6.2050e-01],\n",
      "         [ 5.8074e-01, -1.7359e-01,  9.7745e-01,  ..., -1.1712e-01,\n",
      "           2.4862e+00, -7.3540e-01],\n",
      "         [ 3.6941e-01, -2.4378e-01,  1.3534e-01,  ..., -3.2791e-04,\n",
      "           1.3317e+00, -8.5330e-01]],\n",
      "\n",
      "        [[-6.6126e-01,  1.2536e-01,  3.1147e-01,  ...,  7.6965e-01,\n",
      "           5.1124e-01,  6.7114e-01],\n",
      "         [ 1.3092e+00, -1.6139e-02,  7.5938e-01,  ...,  2.6424e-01,\n",
      "           1.6736e+00, -3.4948e-02],\n",
      "         [ 1.0255e+00,  5.9507e-01,  1.0586e+00,  ...,  8.1532e-02,\n",
      "           6.5934e-01, -2.0619e-01],\n",
      "         ...,\n",
      "         [-1.0830e-01,  1.6901e-01,  1.7231e-01,  ...,  5.8342e-01,\n",
      "          -4.4072e-01, -1.3128e+00],\n",
      "         [-2.0082e-01,  4.0646e-01,  4.0321e-01,  ...,  1.1327e+00,\n",
      "          -2.5185e-01, -7.7994e-01],\n",
      "         [-2.5943e-01,  1.3409e-01,  4.1148e-01,  ...,  1.1608e+00,\n",
      "          -7.2003e-01, -9.8052e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.3602e-02, -9.6242e-02, -4.1562e-01,  ...,  1.2844e-01,\n",
      "          -2.4674e-01,  1.3812e+00],\n",
      "         [-3.1096e-03, -2.5374e-01, -9.3664e-01,  ...,  5.0607e-01,\n",
      "           4.2012e-01, -9.6035e-02],\n",
      "         [ 1.5695e-01,  3.4462e-01, -8.6171e-01,  ...,  7.5528e-01,\n",
      "           2.0730e-01,  2.2908e-01],\n",
      "         ...,\n",
      "         [-6.3925e-01,  1.2804e-01,  4.5515e-01,  ..., -3.2084e-01,\n",
      "          -8.8788e-01, -1.2493e+00],\n",
      "         [-1.2578e-01,  1.0888e+00,  1.8171e-01,  ..., -4.4561e-01,\n",
      "           3.1005e-01, -1.0937e+00],\n",
      "         [ 4.9093e-01,  2.3875e-02,  6.2837e-01,  ...,  9.8530e-02,\n",
      "           5.3486e-02, -1.4427e+00]],\n",
      "\n",
      "        [[ 4.5655e-01,  1.5296e+00,  1.2555e-01,  ..., -7.5729e-01,\n",
      "           1.3644e+00,  3.1962e-01],\n",
      "         [ 1.9833e-01,  3.7101e-01, -3.2098e-01,  ..., -6.4240e-01,\n",
      "           8.7430e-01,  9.4054e-02],\n",
      "         [ 3.4532e-01,  3.7448e-01, -2.9451e-01,  ..., -4.4213e-01,\n",
      "           8.6489e-01,  8.0708e-02],\n",
      "         ...,\n",
      "         [ 5.0628e-01,  2.1838e-01, -6.2392e-01,  ..., -3.1974e-01,\n",
      "           2.9886e-01,  2.1349e-01],\n",
      "         [-5.1886e-01,  1.3000e-01,  2.9470e-01,  ...,  6.6756e-01,\n",
      "           1.7025e+00,  3.7678e-01],\n",
      "         [ 3.9711e-01,  5.4397e-01,  5.1468e-01,  ...,  2.4951e-02,\n",
      "           1.0082e+00,  3.5253e-01]],\n",
      "\n",
      "        [[-6.4608e-01,  1.6275e+00, -1.0151e+00,  ..., -4.8886e-01,\n",
      "          -6.8336e-01,  1.4557e+00],\n",
      "         [-2.7424e-01, -4.0509e-01,  6.1517e-01,  ...,  1.3044e+00,\n",
      "           1.8511e+00, -1.1177e+00],\n",
      "         [ 3.7503e-01, -5.2273e-01,  4.8924e-01,  ...,  9.8421e-01,\n",
      "           1.6626e+00, -9.2499e-01],\n",
      "         ...,\n",
      "         [-2.0660e-01,  3.3368e-01, -3.2161e-01,  ..., -5.9918e-01,\n",
      "          -2.5322e-01, -1.4540e-02],\n",
      "         [ 1.5804e-01,  5.2666e-01, -6.7897e-01,  ...,  9.2051e-02,\n",
      "          -4.7229e-01,  8.9082e-01],\n",
      "         [-3.7214e-01, -8.2696e-02,  1.2376e-01,  ..., -7.0528e-02,\n",
      "          -8.0694e-01,  8.7013e-01]]], device='cuda:0')\n",
      "\n",
      "#C C swims in the\n",
      "#C C looks around the boat\n",
      "#C C closes the door\n",
      "#C C looks around the compound\n",
      "#C C looks around\n",
      "#C C looks around\n",
      "#C C looks around\n",
      "#C C mixes the seeds\n",
      "tensor([32, 33, 34, 35, 36])\n",
      "int64\n",
      "tensor([[[ 5.1929e-01,  2.3013e+00, -7.5649e-01,  ..., -1.6119e+00,\n",
      "          -6.6103e-01,  9.7902e-01],\n",
      "         [-5.7251e-01,  9.1211e-01,  1.0256e+00,  ..., -2.0639e-01,\n",
      "           4.9841e-01, -8.9012e-01],\n",
      "         [-5.0120e-01,  6.0665e-01,  1.0675e+00,  ..., -3.0693e-01,\n",
      "           2.5342e-01, -8.4488e-01],\n",
      "         ...,\n",
      "         [ 4.5826e-01, -1.7680e-01, -5.7339e-02,  ...,  2.7282e-01,\n",
      "           7.1805e-01,  4.8347e-01],\n",
      "         [ 1.5176e-01, -3.9859e-01, -1.9277e-01,  ..., -1.0505e+00,\n",
      "           1.2370e+00, -4.5636e-01],\n",
      "         [ 6.7058e-02, -4.4145e-01,  6.3559e-02,  ..., -4.6662e-01,\n",
      "           2.2115e+00, -7.0692e-01]],\n",
      "\n",
      "        [[ 1.0247e+00,  1.9895e+00, -1.1329e+00,  ..., -9.6771e-01,\n",
      "           2.7044e-01,  1.0252e+00],\n",
      "         [ 8.7557e-01,  6.9174e-01, -1.8189e-01,  ..., -7.9054e-01,\n",
      "           1.4157e+00, -1.9360e-01],\n",
      "         [ 8.4947e-01, -4.6928e-02,  2.8373e-01,  ..., -1.5392e-01,\n",
      "           5.9509e-01, -3.9412e-01],\n",
      "         ...,\n",
      "         [ 5.8963e-01, -5.0655e-02, -4.4237e-02,  ..., -8.7472e-01,\n",
      "           1.6384e+00,  1.4321e-01],\n",
      "         [ 3.7713e-01, -8.6140e-01, -6.0829e-01,  ..., -1.4562e+00,\n",
      "           1.4993e+00, -3.6802e-01],\n",
      "         [ 1.0612e+00, -2.4797e-01, -2.4647e-01,  ..., -6.0328e-01,\n",
      "           1.9371e+00, -4.5197e-01]],\n",
      "\n",
      "        [[ 1.0437e+00,  1.1413e+00, -7.9192e-01,  ..., -2.4760e+00,\n",
      "           1.2967e-01,  3.0196e-02],\n",
      "         [ 4.3909e-01,  3.1480e-01,  8.7959e-01,  ...,  4.3779e-01,\n",
      "          -5.0228e-01, -6.0231e-01],\n",
      "         [ 4.9831e-01,  4.0000e-01,  6.0592e-01,  ..., -5.8657e-01,\n",
      "          -2.1103e-02, -4.3264e-02],\n",
      "         ...,\n",
      "         [ 8.8740e-01,  9.0183e-01, -1.8742e-01,  ..., -6.1969e-01,\n",
      "           4.9779e-01, -8.2272e-01],\n",
      "         [ 5.1790e-01,  1.3563e+00, -3.5025e-02,  ..., -9.0415e-01,\n",
      "           1.1418e+00, -8.1575e-01],\n",
      "         [ 3.8223e-01,  7.5874e-01,  8.9684e-02,  ..., -1.0310e+00,\n",
      "           9.8817e-01, -5.0898e-01]],\n",
      "\n",
      "        [[ 4.9059e-01,  1.1893e+00, -2.9108e-01,  ..., -1.9210e-01,\n",
      "           1.2718e+00,  3.2095e-01],\n",
      "         [ 1.0653e+00,  3.7164e-01, -2.8717e-01,  ...,  8.3420e-01,\n",
      "           1.1808e+00,  6.1543e-01],\n",
      "         [ 8.3614e-01,  5.4288e-02,  4.0085e-02,  ...,  6.4928e-01,\n",
      "           9.9234e-01,  4.3045e-01],\n",
      "         ...,\n",
      "         [-1.1474e+00,  8.4912e-01,  6.5922e-01,  ..., -1.4942e-03,\n",
      "           6.0547e-01, -6.4470e-01],\n",
      "         [-1.0266e+00,  3.6827e-01,  5.5789e-01,  ..., -9.1229e-03,\n",
      "           2.0016e-01, -5.2778e-01],\n",
      "         [-1.6000e+00,  8.2870e-01,  8.5851e-01,  ...,  2.1290e-01,\n",
      "           4.6567e-01, -8.6181e-01]],\n",
      "\n",
      "        [[-9.2148e-01,  7.7370e-01, -4.1717e-01,  ..., -2.4608e-01,\n",
      "           9.9854e-01, -1.2576e+00],\n",
      "         [-5.8553e-01,  1.8961e+00,  2.4350e-01,  ...,  2.6975e-01,\n",
      "           1.9320e+00, -8.7047e-01],\n",
      "         [-6.1376e-01,  1.7995e+00,  1.7982e-01,  ...,  1.6507e-01,\n",
      "           1.7416e+00, -4.8570e-01],\n",
      "         ...,\n",
      "         [ 9.4892e-03,  1.0038e+00,  1.5137e-01,  ..., -4.6034e-01,\n",
      "          -3.4039e-03, -7.0003e-01],\n",
      "         [ 1.3164e-01,  1.1074e+00,  6.9950e-02,  ..., -4.9858e-01,\n",
      "           2.7318e-01, -9.1194e-01],\n",
      "         [ 2.3472e-01,  8.8842e-01,  2.7327e-01,  ..., -1.2501e-01,\n",
      "           3.8231e-01, -7.5284e-01]]], device='cuda:0')\n",
      "\n",
      "#C C lights the fire\n",
      "#C C looks around the fire\n",
      "#C C looks around\n",
      "#C C looks around the compound\n",
      "#C C looks at the mirror\n"
     ]
    }
   ],
   "source": [
    "jj = 0\n",
    "with torch.no_grad():\n",
    "    for data_iter, samples in enumerate(data_loader):\n",
    "        indices = samples['index']\n",
    "        print(samples['index'])\n",
    "        if hasattr(model, \"vision_model\"):\n",
    "            image = samples[\"video_features\"].permute(0, 2, 1, 3, 4).contiguous().cuda()  # BCTHW -> BTCHW\n",
    "            samples[\"video_features\"] = model.vision_model.forward_features(image, use_checkpoint=old_args.use_checkpoint, cls_at_last=False)  # NLD\n",
    "            #print(samples[\"video_features\"])\n",
    "            \n",
    "            # Get tensor and its shape\n",
    "            tensor = samples[\"video_features\"]\n",
    "            tensor_shape = tensor.shape\n",
    "\n",
    "            # Serialize the tensor to bytes and then encode it in base64 to make it JSON-compatible\n",
    "            tensor_bytes = tensor.cpu().numpy().tobytes()\n",
    "            tensor_base64 = base64.b64encode(tensor_bytes).decode('utf-8')  # base64 encoded string\n",
    "\n",
    "            # Get tensor and its shape\n",
    "            index_tensor = samples['index']\n",
    "            print(index_tensor.numpy().dtype)\n",
    "            index_tensor_shape = index_tensor.shape\n",
    "\n",
    "            # Serialize the tensor to bytes and then encode it in base64 to make it JSON-compatible\n",
    "            index_tensor_bytes = index_tensor.cpu().numpy().tobytes()\n",
    "            index_tensor_base64 = base64.b64encode(index_tensor_bytes).decode('utf-8')  # base64 encoded string \n",
    "            \n",
    "            # Prepare the data to send\n",
    "            data = {\n",
    "                \"feature_tensor\": tensor_base64,\n",
    "                \"feature_tensor_shape\": list(tensor_shape),\n",
    "                \"index_tensor\": index_tensor_base64,\n",
    "                \"index_tensor_shape\": list(index_tensor_shape),\n",
    "                \"jj\": jj\n",
    "            }\n",
    "\n",
    "            print(tensor)\n",
    "            \n",
    "            #Send the tensor and shape to the server in the body of a POST request\n",
    "           \n",
    "            response = requests.post(\n",
    "                \"http://127.0.0.1:8000/upload_tensor\",\n",
    "                json=data  # send as JSON body\n",
    "            )\n",
    "\n",
    "            print(response.json())\n",
    "            jj+=1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1457d00-29ea-4e6d-bbff-d4af2bede501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2733a213-6abc-4e8b-96e4-adeb09ca0505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
